{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64a0cd4e",
   "metadata": {},
   "source": [
    "\n",
    "# Text Summarization Using Pre-trained Models\n",
    "**Goal:** Generate concise summaries from long news articles using pre-trained encoderâ€“decoder models.\n",
    "\n",
    "**Features**\n",
    "- Load **CNN/DailyMail** (Kaggle) or your own CSV (`text`, `summary`)\n",
    "- **Abstractive** summarization with Hugging Face Transformers (BART/T5/Pegasus; picks the first two available)\n",
    "- Long-text handling: **truncate** to model limits and optional **chunk-and-merge** pipeline\n",
    "- **ROUGE** evaluation (ROUGE-1/2/L via `rouge-score`)\n",
    "- **Extractive** summarization with **TextRank** (pure Python: TF-IDF + cosine + PageRank)\n",
    "- **Fine-tuning** a pre-trained summarizer on a custom dataset (minimal, Trainer API)\n",
    "\n",
    "**Tools/Libraries:** `transformers`, `pandas`, `torch`, `rouge-score`, `numpy`, `matplotlib`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fed1c8",
   "metadata": {},
   "source": [
    "## 1) Install dependencies & select models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b432e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment as needed in your environment:\n",
    "# %pip install transformers datasets accelerate rouge-score torch pandas numpy matplotlib nltk PyPDF2 python-docx\n",
    "# python -m nltk.downloader punkt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc026671",
   "metadata": {},
   "source": [
    "## 2) Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d35b1c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ainize/bart-base-cnn', 'google-t5/t5-small']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os, json, math, warnings, re\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "# Ensure NLTK sentence tokenizer data is available (handles newer nltk punkt_tab)\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    pass  # we'll download below\n",
    "\n",
    "# Prefer a local data directory inside the workspace to avoid permission issues\n",
    "NLTK_DATA_DIR = os.path.join(os.getcwd(), 'nltk_data')\n",
    "if NLTK_DATA_DIR not in nltk.data.path:\n",
    "    nltk.data.path.append(NLTK_DATA_DIR)\n",
    "\n",
    "for pkg in (\"punkt\", \"punkt_tab\"):\n",
    "    try:\n",
    "        nltk.data.find(f\"tokenizers/{pkg}\")\n",
    "    except LookupError:\n",
    "        try:\n",
    "            nltk.download(pkg, download_dir=NLTK_DATA_DIR, quiet=True)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Could not download nltk resource '{pkg}': {e}\")\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# Pick two abstractive models in priority order (will try to load; fallback to next)\n",
    "CANDIDATE_MODELS = [\n",
    "    \"ainize/bart-base-cnn\",\n",
    "    \"google-t5/t5-small\"\n",
    "    # \"facebook/bart-large-cnn\",\n",
    "    # \"google/pegasus-cnn_dailymail\",\n",
    "]\n",
    "\n",
    "def try_load_model(name: str):\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(name)\n",
    "        mdl = AutoModelForSeq2SeqLM.from_pretrained(name).to(DEVICE)\n",
    "        return tok, mdl\n",
    "    except Exception as e:\n",
    "        warnings.warn(f\"Could not load {name}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "MODELS = []\n",
    "for m in CANDIDATE_MODELS:\n",
    "    tok, mdl = try_load_model(m)\n",
    "    if tok is not None and mdl is not None:\n",
    "        MODELS.append((m, tok, mdl))\n",
    "    if len(MODELS) == 2:\n",
    "        break\n",
    "\n",
    "if not MODELS:\n",
    "    raise RuntimeError(\"No summarization models could be loaded. Install at least one from CANDIDATE_MODELS.\")\n",
    "elif len(MODELS) == 1:\n",
    "    # Duplicate the only one available for comparison display purposes\n",
    "    MODELS = MODELS * 2\n",
    "\n",
    "[m[0] for m in MODELS]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f413ee67",
   "metadata": {},
   "source": [
    "## 3) Load Data (CNN/DailyMail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d0053f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
      "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
      "2  A drunk driver who killed a young woman in a h...   \n",
      "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
      "4  Fleetwood are the only team still to have a 10...   \n",
      "\n",
      "                                             summary  \n",
      "0  Bishop John Folda, of North Dakota, is taking ...  \n",
      "1  Criminal complaint: Cop used his role to help ...  \n",
      "2  Craig Eccleston-Todd, 27, had drunk at least t...  \n",
      "3  Nina dos Santos says Europe must be ready to a...  \n",
      "4  Fleetwood top of League One after 2-0 win at S...  \n",
      "\n",
      "Rows: 287113\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_dataset() -> pd.DataFrame:\n",
    "    # Try CSV files\n",
    "    paths = [\"D:/elevvo-mine/datasets/cnn_dailymail/train.csv\"]\n",
    "    for p in paths:\n",
    "        if os.path.exists(p):\n",
    "            df = pd.read_csv(p)\n",
    "            break\n",
    "        else:\n",
    "            # Try jsonl\n",
    "            if os.path.exists(\"news.jsonl\"):\n",
    "                df = pd.read_json(\"news.jsonl\", lines=True)\n",
    "            else:\n",
    "                raise FileNotFoundError(\"Provide 'news.csv' (text, summary) or 'cnn_dailymail.csv' (article, highlights) or 'news.jsonl'.\")\n",
    "\n",
    "    # Map columns\n",
    "    col_map = {c.lower(): c for c in df.columns}\n",
    "    if \"text\" in col_map:\n",
    "        text_col = col_map[\"text\"]\n",
    "    elif \"article\" in col_map:\n",
    "        text_col = col_map[\"article\"]\n",
    "    else:\n",
    "        raise ValueError(\"Could not find a text/article column. Add 'text' or 'article'.\")\n",
    "\n",
    "    if \"summary\" in col_map:\n",
    "        sum_col = col_map[\"summary\"]\n",
    "    elif \"highlights\" in col_map:\n",
    "        sum_col = col_map[\"highlights\"]\n",
    "    else:\n",
    "        sum_col = None  # evaluation will be limited if not provided\n",
    "\n",
    "    if sum_col is not None:\n",
    "        df = df[[text_col, sum_col]].rename(columns={text_col:\"text\", sum_col:\"summary\"}).dropna()\n",
    "    else:\n",
    "        df = df[[text_col]].rename(columns={text_col:\"text\"}).dropna()\n",
    "\n",
    "    # Trim whitespace\n",
    "    df[\"text\"] = df[\"text\"].astype(str).str.strip()\n",
    "    if \"summary\" in df.columns:\n",
    "        df[\"summary\"] = df[\"summary\"].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "df = load_dataset()\n",
    "print(df.head())\n",
    "print(\"\\nRows:\", len(df))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0027eae",
   "metadata": {},
   "source": [
    "## 4) Preprocess Long Texts & Truncate/Chunk to Model Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "351482ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model A: ainize/bart-base-cnn\n",
      "Sample summary (first 1000 chars of input):\n",
      "Bishop John Folda of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus .\n",
      "The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion .\n",
      "Bishop John Folda of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus .\n",
      "The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion .\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def safe_sent_tokenize(text: str) -> List[str]:\n",
    "    \"\"\"Try NLTK sent_tokenize; if resources missing, fall back to a simple regex splitter.\"\"\"\n",
    "    try:\n",
    "        return sent_tokenize(text)\n",
    "    except LookupError:\n",
    "        # Very lightweight fallback, may not be perfect but avoids crashing\n",
    "        return [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", text) if s.strip()]\n",
    "\n",
    "\n",
    "def chunk_text(text: str, tokenizer, max_input_tokens: int = 1024, overlap_sentences: int = 1) -> List[str]:\n",
    "    \"\"\"Split long text into sentence chunks that fit the model's input token limit.\n",
    "    Uses sentence tokenization + greedy packing by tokens.\n",
    "    \"\"\"\n",
    "    sents = safe_sent_tokenize(text)\n",
    "    chunks, buf = [], []\n",
    "    # Keep a small sliding overlap of sentences between chunks (if requested)\n",
    "    for s in sents:\n",
    "        candidate = \" \".join(buf + [s])\n",
    "        if len(tokenizer.encode(candidate, truncation=False)) <= max_input_tokens:\n",
    "            buf.append(s)\n",
    "        else:\n",
    "            if buf:\n",
    "                chunks.append(\" \".join(buf))\n",
    "                if overlap_sentences > 0:\n",
    "                    # take the last k sentences from the previous buffer as overlap\n",
    "                    buf = buf[-overlap_sentences:]\n",
    "                else:\n",
    "                    buf = []\n",
    "                buf.append(s)\n",
    "            else:\n",
    "                # sentence longer than limit: hard truncate tokens\n",
    "                tokens = tokenizer.encode(s, truncation=True, max_length=max_input_tokens)\n",
    "                chunks.append(tokenizer.decode(tokens, skip_special_tokens=True))\n",
    "                buf = []\n",
    "    if buf:\n",
    "        chunks.append(\" \".join(buf))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def summarize_chunks(chunks: List[str], tokenizer, model, summary_max_tokens=128) -> str:\n",
    "    partial_summaries = []\n",
    "    for ch in chunks:\n",
    "        inputs = tokenizer(\n",
    "            ch,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=tokenizer.model_max_length if tokenizer.model_max_length <= 2048 else 1024\n",
    "        ).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            gen_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=summary_max_tokens,\n",
    "                num_beams=4,\n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        partial = tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "        partial_summaries.append(partial)\n",
    "    # Merge partial summaries (simple join; you could re-summarize the concatenation)\n",
    "    merged = \" \".join(partial_summaries)\n",
    "    return merged\n",
    "\n",
    "\n",
    "def abstractive_summarize(text: str, tokenizer, model, max_input_tokens=1024, summary_max_tokens=128, use_chunking=True) -> str:\n",
    "    tokens = tokenizer.encode(text, truncation=False)\n",
    "    if len(tokens) <= max_input_tokens or not use_chunking:\n",
    "        inputs = tokenizer(\n",
    "            text, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens\n",
    "        ).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            gen_ids = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=summary_max_tokens,\n",
    "                num_beams=4,\n",
    "                length_penalty=1.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        return tokenizer.decode(gen_ids[0], skip_special_tokens=True)\n",
    "    else:\n",
    "        chunks = chunk_text(text, tokenizer, max_input_tokens=max_input_tokens)\n",
    "        return summarize_chunks(chunks, tokenizer, model, summary_max_tokens=summary_max_tokens)\n",
    "\n",
    "# Quick smoke test on one sample\n",
    "sample_text = df.iloc[0][\"text\"]\n",
    "model_name_a, tok_a, mdl_a = MODELS[0]\n",
    "print(\"Model A:\", model_name_a)\n",
    "print(\"Sample summary (first 1000 chars of input):\")\n",
    "print(abstractive_summarize(sample_text[:1000], tok_a, mdl_a, max_input_tokens=1024, summary_max_tokens=64, use_chunking=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a49f8c",
   "metadata": {},
   "source": [
    "## 5) Generate Summaries (Compare Two Models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29db6991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generating with ainize/bart-base-cnn ...\n",
      "\n",
      "Generating with google-t5/t5-small ...\n",
      "\n",
      "Generating with google-t5/t5-small ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (546 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>summary</th>\n",
       "      <th>summary_ainize/bart-base-cnn</th>\n",
       "      <th>summary_google-t5/t5-small</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>By . Associated Press . PUBLISHED: . 14:11 EST...</td>\n",
       "      <td>Bishop John Folda, of North Dakota, is taking ...</td>\n",
       "      <td>Bishop John Folda of the Fargo Catholic Dioces...</td>\n",
       "      <td>the bishop of the Fargo Catholic Diocese in No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(CNN) -- Ralph Mata was an internal affairs li...</td>\n",
       "      <td>Criminal complaint: Cop used his role to help ...</td>\n",
       "      <td>Ralph Mata was an internal affairs lieutenant ...</td>\n",
       "      <td>in the division that investigates allegations ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A drunk driver who killed a young woman in a h...</td>\n",
       "      <td>Craig Eccleston-Todd, 27, had drunk at least t...</td>\n",
       "      <td>Craig Eccleston-Todd, 27, was texting while dr...</td>\n",
       "      <td>Craig Eccleston-Todd, 27, was driving home fro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(CNN) -- With a breezy sweep of his pen Presid...</td>\n",
       "      <td>Nina dos Santos says Europe must be ready to a...</td>\n",
       "      <td>EU facing urgent calls to widen scope of its m...</td>\n",
       "      <td>a new chapter into Crimea's turbulent history,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fleetwood are the only team still to have a 10...</td>\n",
       "      <td>Fleetwood top of League One after 2-0 win at S...</td>\n",
       "      <td>Fleetwood are the only team still to have a 10...</td>\n",
       "      <td>Fleetwood are the only team to have a 100% rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  By . Associated Press . PUBLISHED: . 14:11 EST...   \n",
       "1  (CNN) -- Ralph Mata was an internal affairs li...   \n",
       "2  A drunk driver who killed a young woman in a h...   \n",
       "3  (CNN) -- With a breezy sweep of his pen Presid...   \n",
       "4  Fleetwood are the only team still to have a 10...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Bishop John Folda, of North Dakota, is taking ...   \n",
       "1  Criminal complaint: Cop used his role to help ...   \n",
       "2  Craig Eccleston-Todd, 27, had drunk at least t...   \n",
       "3  Nina dos Santos says Europe must be ready to a...   \n",
       "4  Fleetwood top of League One after 2-0 win at S...   \n",
       "\n",
       "                        summary_ainize/bart-base-cnn  \\\n",
       "0  Bishop John Folda of the Fargo Catholic Dioces...   \n",
       "1  Ralph Mata was an internal affairs lieutenant ...   \n",
       "2  Craig Eccleston-Todd, 27, was texting while dr...   \n",
       "3  EU facing urgent calls to widen scope of its m...   \n",
       "4  Fleetwood are the only team still to have a 10...   \n",
       "\n",
       "                          summary_google-t5/t5-small  \n",
       "0  the bishop of the Fargo Catholic Diocese in No...  \n",
       "1  in the division that investigates allegations ...  \n",
       "2  Craig Eccleston-Todd, 27, was driving home fro...  \n",
       "3  a new chapter into Crimea's turbulent history,...  \n",
       "4  Fleetwood are the only team to have a 100% rec...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "N_SAMPLES = min(10, len(df))  # smaller for faster rerun; adjust as needed\n",
    "subset = df.iloc[:N_SAMPLES].copy()\n",
    "\n",
    "# quick sanity for sentence tokenizer\n",
    "_ = safe_sent_tokenize(subset.iloc[0][\"text\"])[:3]\n",
    "\n",
    "results = []\n",
    "for (model_name, tok, mdl) in MODELS:\n",
    "    print(f\"\\nGenerating with {model_name} ...\")\n",
    "    summaries = []\n",
    "    for txt in subset[\"text\"].tolist():\n",
    "        s = abstractive_summarize(txt, tok, mdl, max_input_tokens=1024, summary_max_tokens=128, use_chunking=True)\n",
    "        summaries.append(s)\n",
    "    subset[f\"summary_{model_name}\"] = summaries\n",
    "    results.append((model_name, summaries))\n",
    "\n",
    "subset.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5816a006",
   "metadata": {},
   "source": [
    "## 6) Evaluate with ROUGE (1/2/L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53d4e6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE F1 scores:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>rouge1</th>\n",
       "      <th>rouge2</th>\n",
       "      <th>rougeL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ainize/bart-base-cnn</td>\n",
       "      <td>0.4113</td>\n",
       "      <td>0.2144</td>\n",
       "      <td>0.3097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>google-t5/t5-small</td>\n",
       "      <td>0.3460</td>\n",
       "      <td>0.1260</td>\n",
       "      <td>0.2282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  rouge1  rouge2  rougeL\n",
       "0  ainize/bart-base-cnn  0.4113  0.2144  0.3097\n",
       "1    google-t5/t5-small  0.3460  0.1260  0.2282"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "def compute_rouge(refs: List[str], hyps: List[str]) -> Dict[str, float]:\n",
    "    scorer = rouge_scorer.RougeScorer([\"rouge1\",\"rouge2\",\"rougeL\"], use_stemmer=True)\n",
    "    sums = {\"rouge1\":0.0,\"rouge2\":0.0,\"rougeL\":0.0}\n",
    "    n = 0\n",
    "    for r, h in zip(refs, hyps):\n",
    "        if not isinstance(r, str) or not isinstance(h, str) or not r.strip() or not h.strip():\n",
    "            continue\n",
    "        scores = scorer.score(r, h)\n",
    "        for k in sums.keys():\n",
    "            sums[k] += scores[k].fmeasure\n",
    "        n += 1\n",
    "    return {k: (v/max(n,1)) for k,v in sums.items()}\n",
    "\n",
    "if \"summary\" in df.columns:\n",
    "    refs = subset[\"summary\"].astype(str).tolist()\n",
    "    score_rows = []\n",
    "    for (model_name, summaries) in results:\n",
    "        rouge = compute_rouge(refs, summaries)\n",
    "        row = {\"model\": model_name, **{k: round(v,4) for k,v in rouge.items()}}\n",
    "        score_rows.append(row)\n",
    "    scores_df = pd.DataFrame(score_rows).sort_values(\"rougeL\", ascending=False)\n",
    "    print(\"Average ROUGE F1 scores:\")\n",
    "    display(scores_df)\n",
    "else:\n",
    "    print(\"No reference summaries found â€” skipping ROUGE. Provide a 'summary' column to evaluate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b9ea1",
   "metadata": {},
   "source": [
    "## 7) Extractive Summarization (TextRank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae99adfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Extractive (TextRank) ---\n",
      " 14:11 EST, 25 October 2013 . 15:36 EST, 25 October 2013 . The bishop of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus in late September and early October. Bishop John Folda (pictured) of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A . Fargo Catholic Diocese in North Dakota (pictured) is where the bishop is located . ...\n",
      "\n",
      "--- Abstractive (ainize/bart-base-cnn) ---\n",
      " Bishop John Folda of the Fargo Catholic Diocese in North Dakota has exposed potentially hundreds of church members in Fargo, Grand Forks and Jamestown to the hepatitis A virus .\n",
      "The state Health Department has issued an advisory of exposure for anyone who attended five churches and took communion . ...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "\n",
    "\n",
    "def textrank_summarize(text: str, top_k_sentences: int = 5) -> str:\n",
    "    sentences = safe_sent_tokenize(text)\n",
    "    if len(sentences) <= top_k_sentences:\n",
    "        return text\n",
    "\n",
    "    # TF-IDF sentence embeddings\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    X = tfidf.fit_transform(sentences)\n",
    "    sim = cosine_similarity(X)\n",
    "\n",
    "    # Build graph and rank sentences\n",
    "    np.fill_diagonal(sim, 0.0)\n",
    "    graph = nx.from_numpy_array(sim)\n",
    "    scores = nx.pagerank(graph)\n",
    "\n",
    "    ranked = sorted(((scores[i], s, i) for i, s in enumerate(sentences)), reverse=True)\n",
    "    # Keep original order for the selected sentences\n",
    "    selected = sorted(ranked[:top_k_sentences], key=lambda x: x[2])\n",
    "    summary = \" \".join([s for _, s, _ in selected])\n",
    "    return summary\n",
    "\n",
    "# Compare extractive vs abstractive on a sample\n",
    "sample = df.iloc[0][\"text\"]\n",
    "extractive = textrank_summarize(sample, top_k_sentences=5)\n",
    "model_name_b, tok_b, mdl_b = MODELS[0]\n",
    "abstractive = abstractive_summarize(sample, tok_b, mdl_b, max_input_tokens=1024, summary_max_tokens=128, use_chunking=True)\n",
    "\n",
    "print(\"\\n--- Extractive (TextRank) ---\\n\", extractive[:800], \"...\")\n",
    "print(\"\\n--- Abstractive ({}) ---\\n\".format(model_name_b), abstractive[:800], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3a24c3",
   "metadata": {},
   "source": [
    "## 8) Fine-tune a Pre-trained Summarizer (Trainer API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d1358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning base model: google-t5/t5-small\n",
      "Training on 160 examples; evaluating on 40 examples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Youssef\\AppData\\Local\\Temp\\ipykernel_18824\\1458014979.py:123: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [50/50 04:06, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.414700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.589900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.256900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.040000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved fine-tuned model to: d:\\elevvo-mine\\summarizer_ft\\final\n",
      "ROUGE after fine-tuning: {'rouge1': 0.3651, 'rouge2': 0.1676, 'rougeL': 0.2575}\n",
      "ROUGE after fine-tuning: {'rouge1': 0.3651, 'rouge2': 0.1676, 'rougeL': 0.2575}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tip: Enable GPU and consider gradient accumulation / mixed precision for speed.\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import DataCollatorForSeq2Seq, Trainer, TrainingArguments\n",
    "from dataclasses import fields as dataclass_fields\n",
    "import inspect\n",
    "import os\n",
    "\n",
    "if \"summary\" not in df.columns:\n",
    "    print(\"Fine-tuning skipped: dataset lacks reference summaries. Provide a 'summary' column.\")\n",
    "else:\n",
    "    FT_MODEL_NAME, FT_TOK, FT_MODEL = MODELS[1]  # choose second model\n",
    "    print(\"Fine-tuning base model:\", FT_MODEL_NAME)\n",
    "\n",
    "    # data configuration\n",
    "    SEED = 42\n",
    "    FT_MAX_SAMPLES = 200   # cap total examples used for fine-tuning\n",
    "    FT_TRAIN_FRAC = 0.8    # split within the small subset\n",
    "    FT_PER_DEVICE_BATCH = 2\n",
    "    FT_MAX_STEPS = 50     # cap steps for a quick run (if supported)\n",
    "\n",
    "    # Make a small, reproducible subset with summaries\n",
    "    df_ft = df.dropna(subset=[\"text\", \"summary\"]).reset_index(drop=True)\n",
    "    n_use = min(FT_MAX_SAMPLES, len(df_ft))\n",
    "    if n_use < 4:\n",
    "        print(f\"Not enough samples to fine-tune (found {len(df_ft)} with summaries). Skipping.\")\n",
    "    else:\n",
    "        df_small = df_ft.sample(n=n_use, random_state=SEED).reset_index(drop=True)\n",
    "\n",
    "        class SummarizationDataset(Dataset):\n",
    "            def __init__(self, df, tokenizer, max_input=1024, max_target=128):\n",
    "                self.df = df.reset_index(drop=True)\n",
    "                self.tokenizer = tokenizer\n",
    "                self.max_input = max_input\n",
    "                self.max_target = max_target\n",
    "            def __len__(self): return len(self.df)\n",
    "            def __getitem__(self, idx):\n",
    "                row = self.df.iloc[idx]\n",
    "                inputs = self.tokenizer(\n",
    "                    row[\"text\"],\n",
    "                    max_length=self.max_input,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                labels = FT_TOK(\n",
    "                    row[\"summary\"],\n",
    "                    max_length=self.max_target,\n",
    "                    truncation=True,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                item = {k: v.squeeze(0) for k,v in inputs.items()}\n",
    "                # Important: replace padding token id's in labels by -100 to ignore loss on padding\n",
    "                labels_ids = labels[\"input_ids\"].squeeze(0)\n",
    "                labels_ids[labels_ids == FT_TOK.pad_token_id] = -100\n",
    "                item[\"labels\"] = labels_ids\n",
    "                return item\n",
    "\n",
    "        # Split small subset\n",
    "        n = len(df_small)\n",
    "        split = max(1, int(n * FT_TRAIN_FRAC))\n",
    "        if split >= n:\n",
    "            split = n - 1  # ensure non-empty eval\n",
    "        train_df = df_small.iloc[:split].copy()\n",
    "        eval_df  = df_small.iloc[split:].copy()\n",
    "\n",
    "        print(f\"Training on {len(train_df)} examples; evaluating on {len(eval_df)} examples.\")\n",
    "\n",
    "        train_ds = SummarizationDataset(train_df, FT_TOK)\n",
    "        eval_ds  = SummarizationDataset(eval_df, FT_TOK)\n",
    "\n",
    "        collator = DataCollatorForSeq2Seq(tokenizer=FT_TOK, model=FT_MODEL)\n",
    "\n",
    "        # Build TrainingArguments kwargs compatible with the installed transformers version\n",
    "        try:\n",
    "            ta_field_names = {f.name for f in dataclass_fields(TrainingArguments)}\n",
    "        except Exception:\n",
    "            ta_field_names = set(inspect.signature(TrainingArguments).parameters.keys())\n",
    "\n",
    "        def add_if_supported(d, key, value):\n",
    "            if key in ta_field_names:\n",
    "                d[key] = value\n",
    "\n",
    "        ta_kwargs = {}\n",
    "        add_if_supported(ta_kwargs, \"output_dir\", \"./summarizer_ft\")\n",
    "        add_if_supported(ta_kwargs, \"learning_rate\", 5e-5)\n",
    "        add_if_supported(ta_kwargs, \"num_train_epochs\", 1)\n",
    "        add_if_supported(ta_kwargs, \"weight_decay\", 0.01)\n",
    "        add_if_supported(ta_kwargs, \"logging_steps\", 10)\n",
    "        add_if_supported(ta_kwargs, \"save_steps\", 50)\n",
    "        add_if_supported(ta_kwargs, \"fp16\", torch.cuda.is_available())\n",
    "        add_if_supported(ta_kwargs, \"seed\", SEED)\n",
    "        add_if_supported(ta_kwargs, \"save_total_limit\", 1)\n",
    "\n",
    "        # Batch size (older versions used per_gpu_* keys)\n",
    "        if \"per_device_train_batch_size\" in ta_field_names:\n",
    "            ta_kwargs[\"per_device_train_batch_size\"] = FT_PER_DEVICE_BATCH\n",
    "            ta_kwargs[\"per_device_eval_batch_size\"] = FT_PER_DEVICE_BATCH\n",
    "        else:\n",
    "            add_if_supported(ta_kwargs, \"per_gpu_train_batch_size\", FT_PER_DEVICE_BATCH)\n",
    "            add_if_supported(ta_kwargs, \"per_gpu_eval_batch_size\", FT_PER_DEVICE_BATCH)\n",
    "\n",
    "        # Evaluation scheduling\n",
    "        if \"evaluation_strategy\" in ta_field_names:\n",
    "            ta_kwargs[\"evaluation_strategy\"] = \"steps\"\n",
    "            add_if_supported(ta_kwargs, \"eval_steps\", 25)\n",
    "            add_if_supported(ta_kwargs, \"predict_with_generate\", True)\n",
    "        else:\n",
    "            # Very old versions\n",
    "            add_if_supported(ta_kwargs, \"evaluate_during_training\", True)\n",
    "\n",
    "        # Cap total training steps for a quick run if supported\n",
    "        add_if_supported(ta_kwargs, \"max_steps\", FT_MAX_STEPS)\n",
    "\n",
    "        # Disable external reporters if supported\n",
    "        add_if_supported(ta_kwargs, \"report_to\", [])\n",
    "\n",
    "        args = TrainingArguments(**ta_kwargs)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model=FT_MODEL,\n",
    "            args=args,\n",
    "            train_dataset=train_ds,\n",
    "            eval_dataset=eval_ds,\n",
    "            data_collator=collator,\n",
    "            tokenizer=FT_TOK\n",
    "        )\n",
    "\n",
    "        # Train (quick run)\n",
    "        trainer.train()\n",
    "\n",
    "        # Save final fine-tuned model in a stable folder for downstream use\n",
    "        final_dir = os.path.join(\"summarizer_ft\", \"final\")\n",
    "        os.makedirs(final_dir, exist_ok=True)\n",
    "        FT_MODEL.save_pretrained(final_dir)\n",
    "        FT_TOK.save_pretrained(final_dir)\n",
    "        print(\"Saved fine-tuned model to:\", os.path.abspath(final_dir))\n",
    "\n",
    "        # After training, quick evaluation with ROUGE on eval set (first few examples)\n",
    "        def generate_batch_summaries(texts, tokenizer, model, max_input=1024, max_new=128):\n",
    "            outs = []\n",
    "            for t in texts:\n",
    "                inputs = tokenizer(t, return_tensors=\"pt\", truncation=True, max_length=max_input).to(DEVICE)\n",
    "                with torch.no_grad():\n",
    "                    gen_ids = model.generate(**inputs, max_new_tokens=max_new, num_beams=4)\n",
    "                outs.append(tokenizer.decode(gen_ids[0], skip_special_tokens=True))\n",
    "            return outs\n",
    "\n",
    "        sample_evals = eval_df.iloc[:min(20, len(eval_df))]\n",
    "        if len(sample_evals) > 0:\n",
    "            preds = generate_batch_summaries(sample_evals[\"text\"].tolist(), FT_TOK, FT_MODEL)\n",
    "            rouge = compute_rouge(sample_evals[\"summary\"].tolist(), preds)\n",
    "            print(\"ROUGE after fine-tuning:\", {k: round(v,4) for k,v in rouge.items()})\n",
    "        else:\n",
    "            print(\"No eval samples to score.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b71e0e3",
   "metadata": {},
   "source": [
    "## 9) Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "248f840f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: summaries_output.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save the subset with generated summaries to a CSV for inspection\n",
    "out_path = \"summaries_output.csv\"\n",
    "subset.to_csv(out_path, index=False)\n",
    "print(\"Saved:\", out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6acb699",
   "metadata": {},
   "source": [
    "## 10) Test Models using a Streamlit Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145b534e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: d:\\elevvo-mine\\summarizer_app.py\n",
      "To launch the app from PowerShell:\n",
      "\n",
      "  streamlit run summarizer_app.py\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:234: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:234: SyntaxWarning: invalid escape sequence '\\s'\n",
      "C:\\Users\\Youssef\\AppData\\Local\\Temp\\ipykernel_12952\\3243301361.py:234: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, time, json\n",
    "from textwrap import dedent\n",
    "\n",
    "# Use the model names already loaded in this notebook (fallback if empty)\n",
    "try:\n",
    "    _model_names = sorted({m[0] for m in MODELS})\n",
    "except Exception:\n",
    "    _model_names = []\n",
    "if not _model_names:\n",
    "    _model_names = [\n",
    "        \"ainize/bart-base-cnn\",\n",
    "        \"google-t5/t5-small\",\n",
    "    ]\n",
    "\n",
    "# Include local fine-tuned checkpoints if present\n",
    "_local_paths = []\n",
    "ft_root = \"summarizer_ft\"\n",
    "final_dir = os.path.join(ft_root, \"final\")\n",
    "if os.path.isdir(final_dir):\n",
    "    _local_paths.append(final_dir)\n",
    "# Also scan for Trainer checkpoints like summarizer_ft/checkpoint-*\n",
    "if os.path.isdir(ft_root):\n",
    "    for name in os.listdir(ft_root):\n",
    "        p = os.path.join(ft_root, name)\n",
    "        if os.path.isdir(p) and name.startswith(\"checkpoint-\"):\n",
    "            # Heuristic: only add if it looks like a HF checkpoint (config.json exists)\n",
    "            if os.path.exists(os.path.join(p, \"config.json\")):\n",
    "                _local_paths.append(p)\n",
    "\n",
    "# Compose candidate list: local paths first, then hub model IDs\n",
    "_candidate_list = _local_paths + _model_names\n",
    "\n",
    "app_code = dedent(f\"\"\"\n",
    "import os, re, time, io\n",
    "import streamlit as st\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Optional parsers\n",
    "try:\n",
    "    import PyPDF2 as _pypdf2\n",
    "except Exception:\n",
    "    _pypdf2 = None\n",
    "try:\n",
    "    from docx import Document as _DocxDocument\n",
    "except Exception:\n",
    "    _DocxDocument = None\n",
    "\n",
    "st.set_page_config(page_title=\"Summarizer\", page_icon=\"ðŸ“\", layout=\"wide\")\n",
    "st.title(\"ðŸ“ Summarizer App\")\n",
    "\n",
    "# Device selection\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "st.sidebar.markdown(f\"**Device:** {{DEVICE}}\")\n",
    "\n",
    "CANDIDATE_MODELS = {json.dumps(_candidate_list)}\n",
    "\n",
    "# Caching compatibility (Streamlit >=1.18 uses cache_resource)\n",
    "def _apply_cache(fn):\n",
    "    if hasattr(st, \"cache_resource\"):\n",
    "        return st.cache_resource(show_spinner=True)(fn)\n",
    "    # Fallback for older versions\n",
    "    return st.cache(allow_output_mutation=True)(fn)\n",
    "\n",
    "def load_model(name: str):\n",
    "    tok = AutoTokenizer.from_pretrained(name)\n",
    "    mdl = AutoModelForSeq2SeqLM.from_pretrained(name).to(DEVICE)\n",
    "    mdl.eval()\n",
    "    return tok, mdl\n",
    "\n",
    "load_model = _apply_cache(load_model)\n",
    "\n",
    "# Safe sentence tokenize (no nltk dependency required at runtime)\n",
    "def safe_sent_tokenize(text: str):\n",
    "    return [s.strip() for s in re.split(r\"(?<=[.!?])\\s+\", text) if s.strip()]\n",
    "\n",
    "# Chunking and summarization helpers\n",
    "\n",
    "def chunk_text(text: str, tokenizer, max_input_tokens: int = 1024, overlap_sentences: int = 1):\n",
    "    sents = safe_sent_tokenize(text)\n",
    "    chunks, buf = [], []\n",
    "    for s in sents:\n",
    "        candidate = \" \".join(buf + [s])\n",
    "        if len(tokenizer.encode(candidate, truncation=False)) <= max_input_tokens:\n",
    "            buf.append(s)\n",
    "        else:\n",
    "            if buf:\n",
    "                chunks.append(\" \".join(buf))\n",
    "                buf = buf[-overlap_sentences:] if overlap_sentences > 0 else []\n",
    "                buf.append(s)\n",
    "            else:\n",
    "                tokens = tokenizer.encode(s, truncation=True, max_length=max_input_tokens)\n",
    "                chunks.append(tokenizer.decode(tokens, skip_special_tokens=True))\n",
    "                buf = []\n",
    "    if buf:\n",
    "        chunks.append(\" \".join(buf))\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def summarize_chunks(chunks, tokenizer, model, summary_max_tokens=128, num_beams=4):\n",
    "    parts = []\n",
    "    for ch in chunks:\n",
    "        inputs = tokenizer(ch, return_tensors=\"pt\", truncation=True, max_length=min(2048, tokenizer.model_max_length)).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            ids = model.generate(**inputs, max_new_tokens=summary_max_tokens, num_beams=num_beams, early_stopping=True)\n",
    "        parts.append(tokenizer.decode(ids[0], skip_special_tokens=True))\n",
    "    return \" \".join(parts)\n",
    "\n",
    "\n",
    "def abstractive_summarize(text: str, tokenizer, model, max_input_tokens=1024, summary_max_tokens=128, use_chunking=True, num_beams=4, use_t5_prefix=False):\n",
    "    if use_t5_prefix:\n",
    "        text = \"summarize: \" + text\n",
    "    tokens = tokenizer.encode(text, truncation=False)\n",
    "    if len(tokens) <= max_input_tokens or not use_chunking:\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=max_input_tokens).to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            ids = model.generate(**inputs, max_new_tokens=summary_max_tokens, num_beams=num_beams, early_stopping=True)\n",
    "        return tokenizer.decode(ids[0], skip_special_tokens=True)\n",
    "    chunks = chunk_text(text, tokenizer, max_input_tokens=max_input_tokens)\n",
    "    return summarize_chunks(chunks, tokenizer, model, summary_max_tokens=summary_max_tokens, num_beams=num_beams)\n",
    "\n",
    "# ---- File upload & extraction ----\n",
    "\n",
    "def _decode_bytes(data: bytes) -> str:\n",
    "    if not isinstance(data, (bytes, bytearray)):\n",
    "        return str(data)\n",
    "    try:\n",
    "        return data.decode(\"utf-8\")\n",
    "    except Exception:\n",
    "        try:\n",
    "            return data.decode(\"latin-1\")\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "\n",
    "def extract_text_from_upload(up_file) -> str:\n",
    "    name = (up_file.name or \"uploaded\").lower()\n",
    "    data = up_file.read()\n",
    "    bio = io.BytesIO(data)\n",
    "    if name.endswith(\".txt\"):\n",
    "        return _decode_bytes(data)\n",
    "    if name.endswith(\".pdf\"):\n",
    "        if _pypdf2 is None:\n",
    "            return \"\"\n",
    "        try:\n",
    "            reader = _pypdf2.PdfReader(bio)\n",
    "            texts = []\n",
    "            for pg in reader.pages:\n",
    "                try:\n",
    "                    t = pg.extract_text() or \"\"\n",
    "                    if t:\n",
    "                        texts.append(t)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            return \" \".join(texts).strip()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    if name.endswith(\".docx\"):\n",
    "        if _DocxDocument is None:\n",
    "            return \"\"\n",
    "        try:\n",
    "            doc = _DocxDocument(bio)\n",
    "            return \" \".join([p.text for p in doc.paragraphs if p.text and p.text.strip()]).strip()\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    return \"\"\n",
    "\n",
    "# Sidebar controls\n",
    "model_name = st.sidebar.selectbox(\"Model\", options=CANDIDATE_MODELS, index=0)\n",
    "max_input_tokens = st.sidebar.slider(\"Max input tokens\", 256, 2048, 1024, 64)\n",
    "summary_max_tokens = st.sidebar.slider(\"Summary max tokens\", 16, 256, 128, 8)\n",
    "num_beams = st.sidebar.slider(\"Beam size\", 1, 6, 4, 1)\n",
    "use_chunking = st.sidebar.checkbox(\"Use chunking for long texts\", value=True)\n",
    "use_t5_prefix = \"t5\" in model_name.lower()\n",
    "\n",
    "# Visual cue if a local fine-tuned model is selected\n",
    "if os.path.isdir(model_name):\n",
    "    st.sidebar.success(\"Using local fine-tuned model\")\n",
    "\n",
    "st.sidebar.caption(\"Tip: T5 models automatically use the 'summarize:' prefix.\")\n",
    "\n",
    "# Backward-compatible primary button\n",
    "\n",
    "def primary_button(label: str):\n",
    "    try:\n",
    "        return st.button(label, type=\"primary\")\n",
    "    except TypeError:\n",
    "        return st.button(label)\n",
    "\n",
    "# File uploader\n",
    "uploaded = st.file_uploader(\"Upload PDF, DOCX, or TXT\", type=[\"pdf\",\"docx\",\"txt\"], accept_multiple_files=True)\n",
    "combined_text = \"\"\n",
    "if uploaded:\n",
    "    texts = []\n",
    "    for f in uploaded:\n",
    "        t = extract_text_from_upload(f)\n",
    "        if t:\n",
    "            texts.append(t)\n",
    "        else:\n",
    "            nm = f.name or \"file\"\n",
    "            if nm.lower().endswith(\".pdf\") and _pypdf2 is None:\n",
    "                st.warning(\"Couldn't read \" + nm + \". Install PyPDF2 to enable PDF parsing: pip install PyPDF2\")\n",
    "            elif nm.lower().endswith(\".docx\") and _DocxDocument is None:\n",
    "                st.warning(\"Couldn't read \" + nm + \". Install python-docx to enable DOCX parsing: pip install python-docx\")\n",
    "            else:\n",
    "                st.warning(\"Couldn't extract text from \" + nm)\n",
    "    if texts:\n",
    "        combined_text = \"  \".join(texts)\n",
    "        st.success(\"Loaded \" + str(len(texts)) + \" file(s)\")\n",
    "        if not st.session_state.get(\"input_text\"):\n",
    "            st.session_state[\"input_text\"] = combined_text[:200000]\n",
    "\n",
    "# Main input area\n",
    "text = st.text_area(\"Enter article text\", height=300, placeholder=\"Paste a long article hereâ€¦\", key=\"input_text\")\n",
    "\n",
    "if primary_button(\"Summarize\"):\n",
    "    if not text or not text.strip():\n",
    "        st.warning(\"Please paste some text or upload a file.\")\n",
    "    else:\n",
    "        with st.spinner(\"Loading model and generatingâ€¦\"):\n",
    "            tok, mdl = load_model(model_name)\n",
    "            t0 = time.time()\n",
    "            out = abstractive_summarize(\n",
    "                text.strip(), tok, mdl,\n",
    "                max_input_tokens=max_input_tokens,\n",
    "                summary_max_tokens=summary_max_tokens,\n",
    "                use_chunking=use_chunking,\n",
    "                num_beams=num_beams,\n",
    "                use_t5_prefix=use_t5_prefix,\n",
    "            )\n",
    "            dt = time.time() - t0\n",
    "        st.success(f\"Done in {{dt:.2f}}s\")\n",
    "        st.subheader(\"Summary\")\n",
    "        st.write(out)\n",
    "\"\"\")\n",
    "\n",
    "out_path = os.path.join(os.getcwd(), \"summarizer_app.py\")\n",
    "with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(app_code)\n",
    "\n",
    "print(\"Wrote:\", out_path)\n",
    "print(\"To launch the app from PowerShell:\")\n",
    "print(\"\\n  streamlit run summarizer_app.py\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b362578",
   "metadata": {},
   "source": [
    "\n",
    "## 11) Summary\n",
    "- Used **pre-trained encoderâ€“decoder models** (BART/T5/Pegasus) for **abstractive** summarization.\n",
    "- Implemented **long-text handling** via truncation and a **chunk-and-merge** strategy.\n",
    "- Evaluated with **ROUGE-1/2/L** (F1) using `rouge-score`.\n",
    "- Implemented **extractive TextRank** and compared outputs qualitatively.\n",
    "- Included a **fine-tuning** template with `Trainer` for your custom dataset.\n",
    "\n",
    "**Next steps:**\n",
    "- Tune generation params (`num_beams`, `min_length`, `length_penalty`) per model.\n",
    "- Add coverage penalty or nucleus sampling for diverse summaries.\n",
    "- For long documents, summarize-by-sections then re-summarize the summaries.\n",
    "- Use `enforce_max_length=False` and careful chunking for models with smaller context.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
