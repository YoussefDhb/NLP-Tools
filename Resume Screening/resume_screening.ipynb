{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27bf1dca",
   "metadata": {},
   "source": [
    "\n",
    "# Resume Screening\n",
    "**Goal:** Build a resume screening system that ranks resumes against job descriptions using embeddings and NLP techniques.  \n",
    "\n",
    "**Features included:**\n",
    "- Parse resumes from **.pdf**, **.docx**, **.txt** files.\n",
    "- Preprocess text and extract entities/skills (spaCy + PhraseMatcher + regex).\n",
    "- Compute embeddings with **sentence-transformers** and rank resumes by **cosine similarity** to job descriptions.\n",
    "- Provide interpretable justifications: matched skills, years of experience, and top sentences that matched.\n",
    "- Optional classifier (train on labeled matches if you have them).\n",
    "- Simple **Streamlit** front-end example to upload a resume and view top matches.\n",
    "- Save results and export top candidates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c544b24",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f719364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment to install required packages in your environment\n",
    "# %pip install sentence-transformers pandas scikit-learn spacy pdfplumber python-docx streamlit\n",
    "# %pip install fuzzywuzzy[speedup]  # optional for fuzzy skill matching\n",
    "# python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14687e41",
   "metadata": {},
   "source": [
    "## 2) Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45018d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\elevvo-mine\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, re, glob, json, math, warnings\n",
    "from collections import Counter, defaultdict\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Resume file parsing\n",
    "import pdfplumber\n",
    "import docx\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (8,4)\n",
    "\n",
    "# Device\n",
    "import torch\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print('Device:', DEVICE)\n",
    "\n",
    "# spaCy model\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except Exception:\n",
    "    nlp = spacy.blank(\"en\")\n",
    "    warnings.warn(\"spaCy model not found; using blank English model. Run `python -m spacy download en_core_web_sm` if needed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81b85ae",
   "metadata": {},
   "source": [
    "## 3) Load Resumes & Jobs Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd6522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded resumes from Resume.csv: 2484\n",
      "Loaded jobs from job_descriptions.csv (first 100000 rows): 100000\n",
      "Loaded jobs from job_descriptions.csv (first 100000 rows): 100000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def strip_html(html: str) -> str:\n",
    "    if not isinstance(html, str) or not html:\n",
    "        return \"\"\n",
    "    # Remove script/style blocks\n",
    "    html = re.sub(r\"<script[\\s\\S]*?</script>\", \" \", html, flags=re.IGNORECASE)\n",
    "    html = re.sub(r\"<style[\\s\\S]*?</style>\", \" \", html, flags=re.IGNORECASE)\n",
    "    # Replace <br> and block tags with newlines\n",
    "    html = re.sub(r\"<(?:br|BR)\\s*/?>\", \"\\n\", html)\n",
    "    html = re.sub(r\"</(?:p|div|section|li|h[1-6]|tr)>\", \"\\n\", html)\n",
    "    # Strip remaining tags\n",
    "    text = re.sub(r\"<[^>]+>\", \" \", html)\n",
    "    # Unescape basic entities\n",
    "    text = (text\n",
    "        .replace(\"&nbsp;\", \" \")\n",
    "        .replace(\"&amp;\", \"&\")\n",
    "        .replace(\"&lt;\", \"<\")\n",
    "        .replace(\"&gt;\", \">\")\n",
    "    )\n",
    "    # Collapse whitespace\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# Use the provided Resume CSV instead of scanning folders\n",
    "resumejobs_base = os.path.join(\"datasets\", \"resumejobs\")\n",
    "resume_csv = os.path.join(resumejobs_base, \"Resume\", \"Resume.csv\")\n",
    "resumes_df = pd.DataFrame()\n",
    "\n",
    "if os.path.exists(resume_csv):\n",
    "    rdf = pd.read_csv(resume_csv)\n",
    "    # Expect columns: ID, Resume_str, Resume_html, Category\n",
    "    lower_map = {c.lower(): c for c in rdf.columns}\n",
    "    id_col = lower_map.get('id')\n",
    "    txt_col = lower_map.get('resume_str')\n",
    "    html_col = lower_map.get('resume_html')\n",
    "    cat_col = lower_map.get('category')\n",
    "\n",
    "    if txt_col is None and html_col is None:\n",
    "        raise ValueError(\"Resume.csv must contain Resume_str or Resume_html column.\")\n",
    "\n",
    "    # Prefer text; fallback to stripped HTML\n",
    "    if txt_col is None:\n",
    "        rdf['__resume_text__'] = rdf[html_col].astype(str).apply(strip_html)\n",
    "    else:\n",
    "        rdf['__resume_text__'] = rdf[txt_col].astype(str)\n",
    "        # If text is missing/empty but HTML exists, fill from HTML\n",
    "        if html_col is not None:\n",
    "            empty_mask = rdf['__resume_text__'].isna() | (rdf['__resume_text__'].str.strip() == '')\n",
    "            rdf.loc[empty_mask, '__resume_text__'] = rdf.loc[empty_mask, html_col].astype(str).apply(strip_html)\n",
    "\n",
    "    # Build unified DataFrame expected by downstream cells\n",
    "    resumes_df = pd.DataFrame({\n",
    "        'path': rdf[id_col] if id_col else rdf.index.astype(str),\n",
    "        'id': rdf[id_col].astype(str) if id_col else rdf.index.astype(str),\n",
    "        'text': rdf['__resume_text__'].astype(str),\n",
    "        'category': rdf[cat_col] if cat_col else ''\n",
    "    })\n",
    "    print(f\"Loaded resumes from Resume.csv: {len(resumes_df)}\")\n",
    "else:\n",
    "    # Fallback to previous folder-based logic if CSV is not present\n",
    "    print(f\"Resume CSV not found at {resume_csv}. Falling back to folder scan.\")\n",
    "\n",
    "    # Helper: read resume text from .txt, .pdf, .docx\n",
    "    def read_txt(path):\n",
    "        with open(path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            return f.read()\n",
    "\n",
    "    def read_pdf(path):\n",
    "        text = []\n",
    "        try:\n",
    "            with pdfplumber.open(path) as pdf:\n",
    "                for page in pdf.pages:\n",
    "                    text.append(page.extract_text() or \"\")\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"pdfplumber failed for {path}: {e}\")\n",
    "        return \"\\n\".join(text)\n",
    "\n",
    "    def read_docx(path):\n",
    "        try:\n",
    "            doc = docx.Document(path)\n",
    "            return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"python-docx failed for {path}: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def read_resume(path):\n",
    "        ext = path.lower().split('.')[-1]\n",
    "        if ext == 'txt':\n",
    "            return read_txt(path)\n",
    "        elif ext == 'pdf':\n",
    "            return read_pdf(path)\n",
    "        elif ext == 'docx':\n",
    "            return read_docx(path)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported resume file type: \" + ext)\n",
    "\n",
    "    # Prefer resumes from datasets/resumejobs/data/data/<CATEGORY>/*.pdf (per attached structure)\n",
    "    resume_root = os.path.join(resumejobs_base, \"data\", \"data\")\n",
    "    resume_paths = []\n",
    "    if os.path.exists(resume_root):\n",
    "        # Collect PDFs first, then optional DOCX/TXT\n",
    "        resume_paths = sorted(glob.glob(os.path.join(resume_root, \"*\", \"*.pdf\")))\n",
    "        resume_paths += sorted(glob.glob(os.path.join(resume_root, \"*\", \"*.docx\")))\n",
    "        resume_paths += sorted(glob.glob(os.path.join(resume_root, \"*\", \"*.txt\")))\n",
    "    else:\n",
    "        # Fallback to local 'resumes/' folder\n",
    "        resume_folder = \"resumes\"\n",
    "        if not os.path.exists(resume_folder):\n",
    "            os.makedirs(resume_folder)\n",
    "            print(\"Created 'resumes/' folder — place your resume files there and re-run the cell.\")\n",
    "        resume_paths = sorted(glob.glob(os.path.join(resume_folder, \"*.*\")))\n",
    "\n",
    "    print(f\"Found {len(resume_paths)} resumes\")\n",
    "\n",
    "    resumes = []\n",
    "    for p in resume_paths:\n",
    "        try:\n",
    "            text = read_resume(p)\n",
    "            category = os.path.basename(os.path.dirname(p)) if os.path.sep in p else \"\"\n",
    "            resumes.append({\n",
    "                \"path\": p,\n",
    "                \"text\": text,\n",
    "                \"id\": os.path.basename(p),\n",
    "                \"category\": category\n",
    "            })\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"Failed to read {p}: {e}\")\n",
    "\n",
    "    resumes_df = pd.DataFrame(resumes)\n",
    "    print(\"Loaded resumes:\", len(resumes_df))\n",
    "\n",
    "# Load jobs dataset: prefer datasets/resumejobs/job_descriptions.csv, else fallback to local jobs.csv\n",
    "jobs_df = None\n",
    "job_csv_candidates = [\n",
    "    os.path.join(resumejobs_base, \"job_descriptions.csv\"),\n",
    "    \"jobs.csv\",\n",
    "]\n",
    "job_csv = next((p for p in job_csv_candidates if os.path.exists(p)), None)\n",
    "\n",
    "if job_csv is not None:\n",
    "    # If using the large attached file, limit to the first 100,000 rows\n",
    "    if os.path.basename(job_csv).lower() == 'job_descriptions.csv':\n",
    "        df = pd.read_csv(job_csv, nrows=100000)\n",
    "        loaded_note = \" (first 100000 rows)\"\n",
    "    else:\n",
    "        df = pd.read_csv(job_csv)\n",
    "        loaded_note = \"\"\n",
    "\n",
    "    # Normalize columns -> 'title', 'description', 'job_id'\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "\n",
    "    desc_keys = [\n",
    "        'job description','description','job_description','jd','desc','text',\n",
    "        'qualifications','requirements','responsibilities','summary'\n",
    "    ]\n",
    "    title_keys = ['job title','title','job_title','position','role']\n",
    "    id_keys = ['job id','job_id','jobid','id']\n",
    "\n",
    "    def pick(keys):\n",
    "        for k in keys:\n",
    "            if k in lower_map:\n",
    "                return lower_map[k]\n",
    "        return None\n",
    "\n",
    "    desc_col = pick(desc_keys)\n",
    "    title_col = pick(title_keys)\n",
    "    id_col = pick(id_keys)\n",
    "\n",
    "    if desc_col is None:\n",
    "        raise ValueError(f\"Could not find a description-like column in {job_csv}. Include one (e.g., 'Job Description' or 'Description').\")\n",
    "\n",
    "    rename_map = {desc_col: 'description'}\n",
    "    if title_col: rename_map[title_col] = 'title'\n",
    "    if id_col: rename_map[id_col] = 'job_id'\n",
    "\n",
    "    df = df.rename(columns=rename_map)\n",
    "\n",
    "    if 'title' not in df.columns:\n",
    "        df['title'] = df.index.astype(str)\n",
    "    if 'job_id' not in df.columns:\n",
    "        df['job_id'] = df.index.astype(str)\n",
    "\n",
    "    jobs_df = df\n",
    "    print(f\"Loaded jobs from {os.path.basename(job_csv)}{loaded_note}: {len(jobs_df)}\")\n",
    "else:\n",
    "    print(\"No job CSV found. Place datasets/resumejobs/job_descriptions.csv or a local jobs.csv with 'title' and 'description'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec612b9",
   "metadata": {},
   "source": [
    "## 4) Preprocess & Extract Entities / Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76555cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skills in matcher: 452\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Basic cleaning\n",
    "def normalize_text(text):\n",
    "    text = text.replace('\\r', ' ').replace('\\n', ' ').strip()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text\n",
    "\n",
    "# Ensure resumes_df has the expected columns\n",
    "required_resume_cols = {'text', 'id'}\n",
    "missing_cols = [c for c in required_resume_cols if c not in resumes_df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"resumes_df missing required columns: {missing_cols}\")\n",
    "\n",
    "resumes_df['text_clean'] = resumes_df['text'].astype(str).apply(normalize_text)\n",
    "if 'jobs_df' in globals() and jobs_df is not None:\n",
    "    # Ensure description_clean exists for later steps\n",
    "    base_desc_col = 'description' if 'description' in jobs_df.columns else jobs_df.columns[0]\n",
    "    jobs_df['description_clean'] = jobs_df[base_desc_col].astype(str).apply(normalize_text)\n",
    "\n",
    "# Regex extractors (email, phone, years)\n",
    "EMAIL_RE = re.compile(r'[\\w\\.\\-+%]+@[\\w\\-]+\\.[\\w\\.-]+')\n",
    "PHONE_RE = re.compile(r'\\+?\\d[\\d\\-\\s()]{6,}\\d')\n",
    "YEXP_RE = re.compile(r'(\\d+)\\+?\\s*(?:years|yrs|y)')\n",
    "\n",
    "def extract_basic_entities(text: str):\n",
    "    emails = EMAIL_RE.findall(text)\n",
    "    phones = PHONE_RE.findall(text)\n",
    "    years = [int(m) for m in YEXP_RE.findall(text)]\n",
    "    return {\"emails\": emails, \"phones\": phones, \"years_mentioned\": years}\n",
    "\n",
    "resumes_df['basic_entities'] = resumes_df['text_clean'].apply(extract_basic_entities)\n",
    "\n",
    "# Skill extraction using PhraseMatcher with a skills list\n",
    "# 1) Load base skills from skills.txt (if present) or fall back to a default list\n",
    "base_skills = []\n",
    "if os.path.exists(\"skills.txt\"):\n",
    "    with open(\"skills.txt\",\"r\",encoding=\"utf-8\") as f:\n",
    "        base_skills = [line.strip() for line in f if line.strip()]\n",
    "else:\n",
    "    # default common skills (extend this file for better matching)\n",
    "    base_skills = [\n",
    "        \"python\",\"pandas\",\"numpy\",\"sql\",\"excel\",\"machine learning\",\"deep learning\",\n",
    "        \"tensorflow\",\"pytorch\",\"scikit-learn\",\"nlp\",\"natural language processing\",\n",
    "        \"data analysis\",\"tableau\",\"power bi\",\"spark\",\"aws\",\"azure\",\"docker\",\"kubernetes\",\n",
    "        \"git\",\"jira\",\"hadoop\",\"bash\",\"linux\",\"java\",\"c++\",\"c#\",\"scala\",\"node.js\",\"react\",\n",
    "        \"flask\",\"django\",\"fastapi\",\"airflow\",\"looker\",\"snowflake\",\"redshift\",\"gcp\",\"bigquery\"\n",
    "    ]\n",
    "\n",
    "# 2) Derive additional candidate skills from job descriptions (if available)\n",
    "derived_skills = []\n",
    "if 'jobs_df' in globals() and jobs_df is not None and len(jobs_df) > 0:\n",
    "    try:\n",
    "        from spacy.lang.en.stop_words import STOP_WORDS as SPACY_STOP\n",
    "    except Exception:\n",
    "        SPACY_STOP = set()\n",
    "    # Custom stopwords to filter generic HR words\n",
    "    EXTRA_STOP = {\n",
    "        'experience','experiences','experienced','responsible','responsibilities','skills','skill',\n",
    "        'ability','abilities','work','working','knowledge','requirements','requirement','role','roles',\n",
    "        'team','teams','strong','excellent','good','great','years','year','including','include','includes',\n",
    "        'etc','must','plus','preferred','nice','fit','position','job','candidate','candidates','company',\n",
    "        'business','clients','customer','support','using','use','used','within','across','based','make','well'\n",
    "    }\n",
    "    STOP = set(w.lower() for w in (SPACY_STOP or set())) | EXTRA_STOP\n",
    "\n",
    "    # Build a corpus from title + description\n",
    "    text_parts = []\n",
    "    if 'title' in jobs_df.columns:\n",
    "        text_parts.append(jobs_df['title'].astype(str).tolist())\n",
    "    desc_col = 'description_clean' if 'description_clean' in jobs_df.columns else ('description' if 'description' in jobs_df.columns else None)\n",
    "    if desc_col:\n",
    "        text_parts.append(jobs_df[desc_col].astype(str).tolist())\n",
    "    corpus = \"\\n\".join([t for sub in text_parts for t in (sub or [])])\n",
    "\n",
    "    # Tokenize with a regex to keep tech strings like c++, c#, node.js\n",
    "    TOKEN_RE = re.compile(r\"[A-Za-z][A-Za-z0-9+.#-]{1,}\")\n",
    "    raw_tokens = [t.lower() for t in TOKEN_RE.findall(corpus)]\n",
    "    tokens = [t for t in raw_tokens if len(t) >= 2 and t not in STOP and not t.isdigit()]\n",
    "\n",
    "    # Unigram and bigram frequencies\n",
    "    uni_counts = Counter(tokens)\n",
    "    bigrams = [f\"{tokens[i]} {tokens[i+1]}\" for i in range(len(tokens)-1)]\n",
    "    bi_counts = Counter(bigrams)\n",
    "\n",
    "    # Uppercase acronyms (kept lower for matcher attr=LOWER)\n",
    "    ACRO_RE = re.compile(r\"\\b[A-Z]{2,}(?:\\.[A-Z]{2,})?\\b\")\n",
    "    acronyms = [a.lower() for a in ACRO_RE.findall(corpus)]\n",
    "\n",
    "    # Keep frequent and skill-like terms\n",
    "    top_unigrams = [w for w, c in uni_counts.most_common(200) if c >= 3]\n",
    "    top_bigrams = [w for w, c in bi_counts.most_common(200) if c >= 2]\n",
    "\n",
    "    # Simple filters to remove overly-generic bigrams\n",
    "    def bigram_ok(bg: str) -> bool:\n",
    "        a, b = bg.split(' ', 1)\n",
    "        return (a not in STOP) and (b not in STOP) and (len(a) > 2 or len(b) > 2)\n",
    "\n",
    "    top_bigrams = [bg for bg in top_bigrams if bigram_ok(bg)]\n",
    "\n",
    "    derived_skills = list(dict.fromkeys(top_unigrams + top_bigrams + acronyms))\n",
    "\n",
    "# 3) Merge and deduplicate while preserving order\n",
    "skills_list = list(dict.fromkeys([*base_skills, *derived_skills]))\n",
    "print(f\"Skills in matcher: {len(skills_list)}\")\n",
    "\n",
    "# Build PhraseMatcher (case-insensitive)\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(s) for s in skills_list]\n",
    "if patterns:\n",
    "    matcher.add(\"SKILL\", patterns)\n",
    "\n",
    "\n",
    "# Optimize: use tokenizer-only doc creation and cap text length\n",
    "MAX_TEXT_CHARS_FOR_SKILLS = 200_000\n",
    "\n",
    "def extract_skills(text: str):\n",
    "    text = text[:MAX_TEXT_CHARS_FOR_SKILLS]\n",
    "    doc = nlp.make_doc(text)\n",
    "    spans = []\n",
    "    for match_id, start, end in matcher(doc):\n",
    "        spans.append(doc[start:end].text)\n",
    "    # frequency and normalized unique list\n",
    "    freq = Counter([s.lower() for s in spans])\n",
    "    return {\"skills\": list(dict.fromkeys([s for s in spans])), \"skills_freq\": dict(freq)}\n",
    "\n",
    "resumes_df['skills'] = resumes_df['text_clean'].apply(extract_skills)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899e1d90",
   "metadata": {},
   "source": [
    "## 5) Compute Embeddings & Rank Resumes by Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1b4fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading embedding model: all-MiniLM-L6-v2\n",
      "Encoding resumes: 2484 (batch=16)\n",
      "Encoding resumes: 2484 (batch=16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 156/156 [01:11<00:00,  2.19it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding jobs: 100000 (batch=8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 12500/12500 [13:29<00:00, 15.44it/s] \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resume embeddings shape: (2484, 384)\n",
      "Job embeddings shape: (100000, 384)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>match_score</th>\n",
       "      <th>matched_skills</th>\n",
       "      <th>years_mentioned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>75329822</td>\n",
       "      <td>69.76</td>\n",
       "      <td>[analyze, social, create, content, media, driv...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15479281</td>\n",
       "      <td>65.71</td>\n",
       "      <td>[analyze, social, create, content, media, metr...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>24677466</td>\n",
       "      <td>65.21</td>\n",
       "      <td>[social, content, media, organizations, brand,...</td>\n",
       "      <td>[17]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11677012</td>\n",
       "      <td>64.92</td>\n",
       "      <td>[analyze, social, media]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>94492380</td>\n",
       "      <td>63.13</td>\n",
       "      <td>[social, create, content, media, drive, brand]</td>\n",
       "      <td>[20, 8, 2]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  match_score                                     matched_skills  \\\n",
       "0  75329822        69.76  [analyze, social, create, content, media, driv...   \n",
       "1  15479281        65.71  [analyze, social, create, content, media, metr...   \n",
       "2  24677466        65.21  [social, content, media, organizations, brand,...   \n",
       "3  11677012        64.92                           [analyze, social, media]   \n",
       "4  94492380        63.13     [social, create, content, media, drive, brand]   \n",
       "\n",
       "  years_mentioned  \n",
       "0              []  \n",
       "1              []  \n",
       "2            [17]  \n",
       "3              []  \n",
       "4      [20, 8, 2]  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Load sentence-transformers model\n",
    "EMB_MODEL_NAME = \"all-MiniLM-L6-v2\"\n",
    "print(\"Loading embedding model:\", EMB_MODEL_NAME)\n",
    "embedder = SentenceTransformer(EMB_MODEL_NAME, device=DEVICE)\n",
    "\n",
    "# Adaptive batch sizes\n",
    "RESUME_BATCH = 128 if DEVICE == \"cuda\" else 16\n",
    "JOB_BATCH = 64 if DEVICE == \"cuda\" else 8\n",
    "\n",
    "# Compute embeddings for resumes\n",
    "if len(resumes_df) > 0:\n",
    "    resume_texts = resumes_df['text_clean'].astype(str).tolist()\n",
    "    print(f\"Encoding resumes: {len(resume_texts)} (batch={RESUME_BATCH})\")\n",
    "    resume_embeddings = embedder.encode(\n",
    "        resume_texts,\n",
    "        batch_size=RESUME_BATCH,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    )\n",
    "else:\n",
    "    resume_embeddings = np.zeros((0, embedder.get_sentence_embedding_dimension()))\n",
    "\n",
    "# Compute embeddings for job descriptions (if available)\n",
    "if 'jobs_df' in globals() and jobs_df is not None and len(jobs_df) > 0:\n",
    "    job_texts = jobs_df['description_clean'].astype(str).tolist()\n",
    "    print(f\"Encoding jobs: {len(job_texts)} (batch={JOB_BATCH})\")\n",
    "    job_embeddings = embedder.encode(\n",
    "        job_texts,\n",
    "        batch_size=JOB_BATCH,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "    )\n",
    "else:\n",
    "    job_embeddings = np.zeros((0, embedder.get_sentence_embedding_dimension()))\n",
    "\n",
    "print(\"Resume embeddings shape:\", resume_embeddings.shape)\n",
    "print(\"Job embeddings shape:\", job_embeddings.shape)\n",
    "\n",
    "# Similarity function\n",
    "def rank_resumes_for_job(job_idx: int, top_k: int = 5):\n",
    "    if job_embeddings.shape[0] == 0:\n",
    "        raise ValueError(\"No job embeddings available. Provide a jobs dataset.\")\n",
    "    job_emb = job_embeddings[job_idx].reshape(1, -1)\n",
    "    sims = cosine_similarity(job_emb, resume_embeddings).squeeze()  # shape (n_resumes,)\n",
    "    idxs = np.argsort(sims)[::-1][:top_k]\n",
    "\n",
    "    # Precompute job words once\n",
    "    job_words = set(\n",
    "        w.lower() for w in re.findall(r'\\w+', jobs_df.iloc[job_idx]['description_clean']) if len(w) > 2\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for i in idxs:\n",
    "        score = float(sims[i])\n",
    "        row = resumes_df.iloc[i].to_dict()\n",
    "        row['match_score'] = round(float(score)*100, 2)\n",
    "        # Get matched skills intersection\n",
    "        rskills = set([s.lower() for s in row.get('skills', {}).get('skills', [])])\n",
    "        matched_skills = list(rskills & job_words)\n",
    "        # Fallback: token-wise overlap between resume skills and job words\n",
    "        if not matched_skills:\n",
    "            def tokens_in_job(s: str) -> bool:\n",
    "                toks = [t.lower() for t in re.findall(r'\\w+', s) if len(t) > 2]\n",
    "                return any(t in job_words for t in toks)\n",
    "            matched_skills = [s for s in rskills if tokens_in_job(s)]\n",
    "        row['matched_skills'] = matched_skills\n",
    "        # years mentioned\n",
    "        row['years_mentioned'] = row.get('basic_entities', {}).get('years_mentioned', [])\n",
    "        rows.append(row)\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Example: rank resumes for the first job\n",
    "if 'jobs_df' in globals() and jobs_df is not None and len(jobs_df)>0 and len(resume_embeddings)>0:\n",
    "    top_matches = rank_resumes_for_job(0, top_k=5)\n",
    "    display(top_matches[['id','match_score','matched_skills','years_mentioned']])\n",
    "else:\n",
    "    print(\"Provide Resume.csv and job_descriptions.csv (or jobs.csv) to run the ranking example.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b88c0f",
   "metadata": {},
   "source": [
    "## 6) Train a classifier on labeled pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b5f8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labeled_path = \"labeled_pairs.csv\"  # expected columns: job_id, resume_id, label\n",
    "if os.path.exists(labeled_path):\n",
    "    pairs = pd.read_csv(labeled_path)\n",
    "    \n",
    "    # Map ids to indices (robust to presence/absence of 'job_id' column)\n",
    "    job_ids_series = jobs_df.get('job_id', jobs_df.index.astype(str))\n",
    "    job_id_to_idx = {str(jid): i for i, jid in enumerate(job_ids_series.astype(str).tolist())}\n",
    "    \n",
    "    # Build multiple resume-id lookup strategies to support both Resume.csv IDs and file-based IDs\n",
    "    res_ids = resumes_df['id'].astype(str).tolist()\n",
    "    resume_by_id = {rid: i for i, rid in enumerate(res_ids)}\n",
    "    resume_by_id_basename = {os.path.basename(rid): i for i, rid in enumerate(res_ids)}\n",
    "    resume_by_path_basename = {}\n",
    "    if 'path' in resumes_df.columns:\n",
    "        paths = resumes_df['path'].astype(str).tolist()\n",
    "        resume_by_path_basename = {os.path.basename(p): i for i, p in enumerate(paths)}\n",
    "    \n",
    "    def map_resume_index(rid_raw) -> Optional[int]:\n",
    "        rid_raw = str(rid_raw).strip()\n",
    "        # Try exact ID, basename of ID (if it looked like a path), and basename of stored path\n",
    "        for candidate in (\n",
    "            resume_by_id.get(rid_raw),\n",
    "            resume_by_id.get(os.path.basename(rid_raw)),\n",
    "            resume_by_id_basename.get(os.path.basename(rid_raw)),\n",
    "            resume_by_path_basename.get(os.path.basename(rid_raw)),\n",
    "):\n",
    "            if candidate is not None:\n",
    "                return candidate\n",
    "        return None\n",
    "    \n",
    "    X = []\n",
    "    y = []\n",
    "    used = 0\n",
    "    for _, row in pairs.iterrows():\n",
    "        jid = str(row['job_id'])\n",
    "        rid_raw = row['resume_id']\n",
    "        jidx = job_id_to_idx.get(jid)\n",
    "        ridx = map_resume_index(rid_raw)\n",
    "        if jidx is None or ridx is None:\n",
    "            continue\n",
    "        je = job_embeddings[jidx]\n",
    "        re_ = resume_embeddings[ridx]\n",
    "        # Include cosine similarity as an explicit scalar feature\n",
    "        cos = float(cosine_similarity(je.reshape(1, -1), re_.reshape(1, -1)).squeeze())\n",
    "        # Feature vector: concat embeddings, elementwise product, abs diff, and cosine similarity\n",
    "        feat = np.concatenate([je, re_, je * re_, np.abs(je - re_), [cos]])\n",
    "        X.append(feat)\n",
    "        y.append(int(row['label']))\n",
    "        used += 1\n",
    "    \n",
    "    if not X:\n",
    "        print(\"No matching (job_id, resume_id) pairs found between labeled_pairs.csv and current datasets — skipping classifier training.\")\n",
    "    else:\n",
    "        X = np.vstack(X)\n",
    "        y = np.array(y)\n",
    "        scaler = StandardScaler()\n",
    "        Xs = scaler.fit_transform(X)\n",
    "        clf = LogisticRegression(max_iter=500)\n",
    "        clf.fit(Xs, y)\n",
    "        print(f\"Trained classifier on {len(y)} labeled pairs. You can now predict match probabilities for new pairs.\")\n",
    "else:\n",
    "    print(\"No labeled_pairs.csv found — skipping classifier training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842ee48b",
   "metadata": {},
   "source": [
    "## 7) Present Top-ranked Resumes with Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bc9edbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOB: Digital Marketing Specialist\n",
      "DESCRIPTION (first 250 chars): Social Media Managers oversee an organizations social media presence. They create and schedule content, engage with followers, and analyze social media metrics to drive brand awareness and engagement. ...\n",
      "\n",
      "Rank 1: Resume 75329822 — Match Score: 69.76%\n",
      "\n",
      "Matched skills: ['analyze', 'social', 'create', 'content', 'media', 'drive', 'brand', 'engage']\n",
      "Top matching sentences from resume:\n",
      "- Social Media Management\n",
      "- Public Relations and Social Media Manager 11/2012 to 06/2014 Company Name Responsible for the execution & management of strategies supporting content development, influencer marketing, events, strategic partnerships, cause marketing and social media campaigns\n",
      "- PUBLIC RELATIONS/SOCIAL MEDIA MANAGEMENT Summary Public Relations Manager with strong communications, event planning, media relations and social media experience within consumer brands\n",
      "\n",
      "---\n",
      "\n",
      "Rank 2: Resume 15479281 — Match Score: 65.71%\n",
      "\n",
      "Matched skills: ['analyze', 'social', 'create', 'content', 'media', 'drive', 'brand', 'engage']\n",
      "Top matching sentences from resume:\n",
      "- Social Media Management\n",
      "- Public Relations and Social Media Manager 11/2012 to 06/2014 Company Name Responsible for the execution & management of strategies supporting content development, influencer marketing, events, strategic partnerships, cause marketing and social media campaigns\n",
      "- PUBLIC RELATIONS/SOCIAL MEDIA MANAGEMENT Summary Public Relations Manager with strong communications, event planning, media relations and social media experience within consumer brands\n",
      "\n",
      "---\n",
      "\n",
      "Rank 2: Resume 15479281 — Match Score: 65.71%\n",
      "\n",
      "Matched skills: ['analyze', 'social', 'create', 'content', 'media', 'metrics', 'brand', 'engage', 'managers']\n",
      "Top matching sentences from resume:\n",
      "- Manage the brand's Social Media platforms such as Facebook, Twitter, Instagram, Pinterest, Tumblr, Youtube, Viva Radio, Snapchat, etc\n",
      "- Plan and execute events, influencer marketing, celebrity outreach, media monitoring, and product placement for social uses\n",
      "- Manager social media accounts with over 6 million followers total\n",
      "\n",
      "---\n",
      "\n",
      "Rank 3: Resume 24677466 — Match Score: 65.21%\n",
      "\n",
      "Matched skills: ['social', 'content', 'media', 'organizations', 'brand', 'managers']\n",
      "Top matching sentences from resume:\n",
      "- Experience Social Media & Communications Manager 07/2011 to Current Company Name City , State Works with chief strategist, as well as on-site managers, to assess communications needs of non-profit organizations Formulates communications plans and social media strategies tailored to client needs Oversees the implementation of communications plans and social media strategies for local non-profit organizations Attends local client events and works with on-site client managers, as well as content development managers, to do live social media posting from client, Agitate Media, and parent company (MAM Squared,LLC) accounts Evaluates campaign results Community Outreach & Involvement Manager (Technical Community Manager) 09/2012 to 01/2015 Company Name City , State Wrote and edited content for company e-newsletters Developed and edited marketing/promotional material and presentation content for special projects and events Updated company website with relevant material using the Kentico content management system Managed social media campaigns to promote engagement amongst 43 technical groups on Facebook, LinkedIn, YouTube, and Twitter Worked collaboratively with IT and Web Department Directors to implement complex website changes and revisions Generated and analyzed monthly engagement data to recommend and implement necessary tactical revisions Served as liaison between membership and high level executive leaders Developed and managed strategic plans to increase engagement & membership within 43 scientific technical groups Managed logistics for special events and activities Managed budget and allocation of funding for special events and activities Marketing Coordinator 10/2008 to 09/2012 Company Name City , State Designed, laid out, wrote, and edited all messaging for company marketing material Managed, maintained, and updated all content on company website Implemented company website redesign, twice Assisted Web vendors with SEO strategy implementation Drove and monitored Web traffic using Google Analytics reports Managed social media efforts on Facebook and Twitter Developed fliers for events and programs Managed, wrote, and edited monthly newsletters for staff, consumers, and board members Worked with Community Outreach Department in providing media support Assisted with the development and execution of communications and marketing plans Organized, publicized, and coordinated staff presence at various internal and external events Youth Department Assistant 01/2008 to 10/2008 Company Name City , State Wrote Web newsletter for an audience of more than 16,000 people, bi-weekly Contributed and implemented creative ideas to revamp Web newsletter Contributed articles to the Homefront Magazine printed publication Edited Homefront Magazine adhering to AP Style rules Processed payroll for nationwide camp staff Updated Web content for nationwide camp website and company intranet using Dreamweaver Responded to nationwide staff and general inquiries in a professional manner, daily Reviewed, analyzed, and evaluated camp financial assistance applications and awarded funds appropriately Monitored \"Planet D\" social media message board for children with Type 1 diabetes, daily Program Coordinator Intern 05/2007 to 08/2007 Company Name City , State Managed all aspects of program; ranging from public relations to budget development, and implementation Created compelling marketing material to advertise program events Developed and implemented program schedule, curriculum, and related events for teens ages 14-17 years old Coordinated and supervised all program functions and special activities; scheduled speakers, tours, structured activities, and project work Constructed annual summary and annual report for the Human Relations Department Public Relations Intern 01/2007 to 05/2007 Company Name City , State Served as a liaison with local press as YMCA representative at local events Ensured press knew where to set-up at local YMCA events Ensured press had all background information and material needed on YMCA at relevant local events Assisted the Financial Development Department with the development of \"The Mayor's Challenge\" campaign, an initiative to fight obesity in Guilford County Continual management of media relations for \"The Mayor's Challenge\" campaign, including PSA development and radio script writing Performed copy editing on local YMCA website Created fliers for various events Public Relations Intern 01/2005 to 05/2005 Company Name City , State Developed feature stories for Athletic Department's website Regularly updated content on Athletic Department's website Interviewed sports coaches and various sports team members for newsletter content Wrote press releases, weekly Technical Skills Adobe Creative Suite, Dreamweaver, FrontPage, Microsoft Office, Microsoft Outlook, Oracle Procurement, Joomla Content Management System, Kentico Content Management System\n",
      "- SOCIAL MEDIA & COMMUNICATIONS MANAGER Education Master of Business Administration : Business 2014 University of Maryland University College City , State Bachelor of Science : Journalism & Mass Communication (Public Relations) 2007 North Carolina Agricultural & Technical State University City , State Summary Dedicated, creative, and highly-motivated communications & marketing professional with exceptional interpersonal skills and over seven years experience in the field\n",
      "- Areas of expertise include Web and print content development and editing, social media content development and monitoring, brand management, and project management, among other skills\n",
      "\n",
      "---\n",
      "\n",
      "Matched skills: ['analyze', 'social', 'create', 'content', 'media', 'metrics', 'brand', 'engage', 'managers']\n",
      "Top matching sentences from resume:\n",
      "- Manage the brand's Social Media platforms such as Facebook, Twitter, Instagram, Pinterest, Tumblr, Youtube, Viva Radio, Snapchat, etc\n",
      "- Plan and execute events, influencer marketing, celebrity outreach, media monitoring, and product placement for social uses\n",
      "- Manager social media accounts with over 6 million followers total\n",
      "\n",
      "---\n",
      "\n",
      "Rank 3: Resume 24677466 — Match Score: 65.21%\n",
      "\n",
      "Matched skills: ['social', 'content', 'media', 'organizations', 'brand', 'managers']\n",
      "Top matching sentences from resume:\n",
      "- Experience Social Media & Communications Manager 07/2011 to Current Company Name City , State Works with chief strategist, as well as on-site managers, to assess communications needs of non-profit organizations Formulates communications plans and social media strategies tailored to client needs Oversees the implementation of communications plans and social media strategies for local non-profit organizations Attends local client events and works with on-site client managers, as well as content development managers, to do live social media posting from client, Agitate Media, and parent company (MAM Squared,LLC) accounts Evaluates campaign results Community Outreach & Involvement Manager (Technical Community Manager) 09/2012 to 01/2015 Company Name City , State Wrote and edited content for company e-newsletters Developed and edited marketing/promotional material and presentation content for special projects and events Updated company website with relevant material using the Kentico content management system Managed social media campaigns to promote engagement amongst 43 technical groups on Facebook, LinkedIn, YouTube, and Twitter Worked collaboratively with IT and Web Department Directors to implement complex website changes and revisions Generated and analyzed monthly engagement data to recommend and implement necessary tactical revisions Served as liaison between membership and high level executive leaders Developed and managed strategic plans to increase engagement & membership within 43 scientific technical groups Managed logistics for special events and activities Managed budget and allocation of funding for special events and activities Marketing Coordinator 10/2008 to 09/2012 Company Name City , State Designed, laid out, wrote, and edited all messaging for company marketing material Managed, maintained, and updated all content on company website Implemented company website redesign, twice Assisted Web vendors with SEO strategy implementation Drove and monitored Web traffic using Google Analytics reports Managed social media efforts on Facebook and Twitter Developed fliers for events and programs Managed, wrote, and edited monthly newsletters for staff, consumers, and board members Worked with Community Outreach Department in providing media support Assisted with the development and execution of communications and marketing plans Organized, publicized, and coordinated staff presence at various internal and external events Youth Department Assistant 01/2008 to 10/2008 Company Name City , State Wrote Web newsletter for an audience of more than 16,000 people, bi-weekly Contributed and implemented creative ideas to revamp Web newsletter Contributed articles to the Homefront Magazine printed publication Edited Homefront Magazine adhering to AP Style rules Processed payroll for nationwide camp staff Updated Web content for nationwide camp website and company intranet using Dreamweaver Responded to nationwide staff and general inquiries in a professional manner, daily Reviewed, analyzed, and evaluated camp financial assistance applications and awarded funds appropriately Monitored \"Planet D\" social media message board for children with Type 1 diabetes, daily Program Coordinator Intern 05/2007 to 08/2007 Company Name City , State Managed all aspects of program; ranging from public relations to budget development, and implementation Created compelling marketing material to advertise program events Developed and implemented program schedule, curriculum, and related events for teens ages 14-17 years old Coordinated and supervised all program functions and special activities; scheduled speakers, tours, structured activities, and project work Constructed annual summary and annual report for the Human Relations Department Public Relations Intern 01/2007 to 05/2007 Company Name City , State Served as a liaison with local press as YMCA representative at local events Ensured press knew where to set-up at local YMCA events Ensured press had all background information and material needed on YMCA at relevant local events Assisted the Financial Development Department with the development of \"The Mayor's Challenge\" campaign, an initiative to fight obesity in Guilford County Continual management of media relations for \"The Mayor's Challenge\" campaign, including PSA development and radio script writing Performed copy editing on local YMCA website Created fliers for various events Public Relations Intern 01/2005 to 05/2005 Company Name City , State Developed feature stories for Athletic Department's website Regularly updated content on Athletic Department's website Interviewed sports coaches and various sports team members for newsletter content Wrote press releases, weekly Technical Skills Adobe Creative Suite, Dreamweaver, FrontPage, Microsoft Office, Microsoft Outlook, Oracle Procurement, Joomla Content Management System, Kentico Content Management System\n",
      "- SOCIAL MEDIA & COMMUNICATIONS MANAGER Education Master of Business Administration : Business 2014 University of Maryland University College City , State Bachelor of Science : Journalism & Mass Communication (Public Relations) 2007 North Carolina Agricultural & Technical State University City , State Summary Dedicated, creative, and highly-motivated communications & marketing professional with exceptional interpersonal skills and over seven years experience in the field\n",
      "- Areas of expertise include Web and print content development and editing, social media content development and monitoring, brand management, and project management, among other skills\n",
      "\n",
      "---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def justify_match(job_idx: int, resume_row: pd.Series, top_sentences: int = 3):\n",
    "    # Find sentences in resume that have highest similarity to job description sentences\n",
    "    job_text = jobs_df.iloc[job_idx].get('description_clean', jobs_df.iloc[job_idx].get('description',''))\n",
    "    # split into sentences\n",
    "    job_sents = [s for s in re.split(r'[\\n\\.!?]+', job_text) if s.strip()][:200]  # cap for speed\n",
    "    res_sents = [s for s in re.split(r'[\\n\\.!?]+', resume_row['text_clean']) if s.strip()][:200]\n",
    "    if not job_sents or not res_sents:\n",
    "        return {\"top_sentences\": [], \"matched_skills\": resume_row.get('matched_skills', [])}\n",
    "    # embed sentences (small number) and compute similarity\n",
    "    emb_job_sents = embedder.encode(job_sents, convert_to_numpy=True)\n",
    "    emb_res_sents = embedder.encode(res_sents, convert_to_numpy=True)\n",
    "    sim = cosine_similarity(emb_res_sents, emb_job_sents)  # res x job\n",
    "    # For each resume sentence, pick max similarity to any job sentence\n",
    "    res_scores = sim.max(axis=1)\n",
    "    top_idx = np.argsort(res_scores)[-top_sentences:][::-1]\n",
    "    top_sents = [res_sents[i].strip() for i in top_idx]\n",
    "    return {\"top_sentences\": top_sents, \"matched_skills\": resume_row.get('matched_skills', [])}\n",
    "\n",
    "# Display nicely for a job\n",
    "def present_top_for_job(job_idx:int, top_k:int=5):\n",
    "    job = jobs_df.iloc[job_idx]\n",
    "    print(\"JOB:\", job.get('title','(no title)'))\n",
    "    desc_preview = job.get('description_clean', job.get('description',''))\n",
    "    print(\"DESCRIPTION (first 250 chars):\", (desc_preview[:250] if isinstance(desc_preview,str) else str(desc_preview)) , \"...\\n\")\n",
    "    matches = rank_resumes_for_job(job_idx, top_k=top_k)\n",
    "    for i, r in matches.iterrows():\n",
    "        print(f\"Rank {i+1}: Resume {r['id']} — Match Score: {r['match_score']}%\\n\")\n",
    "        justification = justify_match(job_idx, r, top_sentences=3)\n",
    "        print(\"Matched skills:\", justification['matched_skills'])\n",
    "        print(\"Top matching sentences from resume:\")\n",
    "        for s in justification['top_sentences']:\n",
    "            print(\"-\", s)\n",
    "        print(\"\\n---\\n\")\n",
    "\n",
    "# Example presentation (if jobs present)\n",
    "if jobs_df is not None and len(jobs_df)>0:\n",
    "    present_top_for_job(0, top_k=3)\n",
    "else:\n",
    "    print(\"No jobs available to present matches.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e13586",
   "metadata": {},
   "source": [
    "## 8) Streamlit Front-end to test models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be62cb21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Streamlit app to resume_screening.py. Run it with: streamlit run resume_screening.py\n"
     ]
    }
   ],
   "source": [
    "# Save this as app.py and run: streamlit run app.py\n",
    "streamlit_app = r\"\"\"\n",
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import os, re, io\n",
    "import numpy as np\n",
    "\n",
    "st.set_page_config(page_title=\"Resume Screening App\", page_icon=\"📄\", layout=\"wide\")\n",
    "st.title('Resume Screening')\n",
    "\n",
    "# Model loading (adjust path if running in separate env)\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    return SentenceTransformer('%s')\n",
    "\n",
    "model = load_model()\n",
    "\n",
    "st.sidebar.header('Job selector')\n",
    "# Prefer datasets/resumejobs/job_descriptions.csv, else fallback to jobs.csv\n",
    "jobs = None\n",
    "job_text = ''\n",
    "\n",
    "def normalize_text(text: str):\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text) if text is not None else ''\n",
    "    return re.sub(r'\\s+', ' ', text.replace('\\r',' ').replace('\\n',' ')).strip()\n",
    "\n",
    "def load_jobs_df():\n",
    "    # Try dataset path first\n",
    "    ds_path = os.path.join('datasets','resumejobs','job_descriptions.csv')\n",
    "    if os.path.exists(ds_path):\n",
    "        df = pd.read_csv(ds_path, nrows=5000)  # cap for snappy UI\n",
    "    elif os.path.exists('jobs.csv'):\n",
    "        df = pd.read_csv('jobs.csv')\n",
    "    else:\n",
    "        return None\n",
    "    lower_map = {c.lower(): c for c in df.columns}\n",
    "    def pick(keys):\n",
    "        for k in keys:\n",
    "            if k in lower_map:\n",
    "                return lower_map[k]\n",
    "        return None\n",
    "    desc_col = pick(['job description','description','job_description','jd','desc','text','summary','requirements','qualifications']) or df.columns[0]\n",
    "    title_col = pick(['job title','title','job_title','position','role'])\n",
    "    id_col = pick(['job id','job_id','jobid','id'])\n",
    "    rename_map = {desc_col: 'description'}\n",
    "    if title_col: rename_map[title_col] = 'title'\n",
    "    if id_col: rename_map[id_col] = 'job_id'\n",
    "    df = df.rename(columns=rename_map)\n",
    "    if 'title' not in df.columns:\n",
    "        df['title'] = df.index.astype(str)\n",
    "    if 'job_id' not in df.columns:\n",
    "        df['job_id'] = df.index.astype(str)\n",
    "    df['description_clean'] = df['description'].astype(str).apply(normalize_text)\n",
    "    return df\n",
    "\n",
    "# Lightweight skill matching\n",
    "@st.cache_resource\n",
    "def load_skills():\n",
    "    skills = []\n",
    "    if os.path.exists('skills.txt'):\n",
    "        with open('skills.txt','r',encoding='utf-8',errors='ignore') as f:\n",
    "            skills = [l.strip() for l in f if l.strip()]\n",
    "    return [s.lower() for s in skills]\n",
    "\n",
    "skills_list = load_skills()\n",
    "\n",
    "jobs = load_jobs_df()\n",
    "if jobs is not None and len(jobs) > 0:\n",
    "    job_idx = st.sidebar.selectbox(\n",
    "        'Choose job',\n",
    "        jobs.index.tolist(),\n",
    "        format_func=lambda i: f\"{jobs.loc[i,'title']} (id={jobs.loc[i,'job_id']})\" if 'job_id' in jobs.columns else f\"{jobs.loc[i,'title']} (row {i})\"\n",
    "    )\n",
    "    job_text = jobs.loc[job_idx, 'description_clean']\n",
    "else:\n",
    "    st.sidebar.info('Place datasets/resumejobs/job_descriptions.csv or jobs.csv in the app folder.')\n",
    "    job_text = st.text_area('Or paste a job description here:')\n",
    "\n",
    "# Sidebar options\n",
    "st.sidebar.header('Options')\n",
    "top_k = st.sidebar.slider('Top-K resumes to display', 1, 50, 10)\n",
    "show_just = st.sidebar.checkbox('Show justifications for top matches', value=False)\n",
    "just_n = st.sidebar.slider('Sentences per resume (when showing justification)', 1, 5, 3)\n",
    "\n",
    "# Readers for uploaded files\n",
    "def read_txt_file(uploaded):\n",
    "    data = uploaded.read()\n",
    "    try:\n",
    "        return data.decode('utf-8', errors='ignore') if isinstance(data, (bytes, bytearray)) else str(data)\n",
    "    finally:\n",
    "        try:\n",
    "            uploaded.seek(0)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def read_pdf_file(uploaded):\n",
    "    text = ''\n",
    "    try:\n",
    "        import pdfplumber\n",
    "        with pdfplumber.open(io.BytesIO(uploaded.read())) as pdf:\n",
    "            for p in pdf.pages:\n",
    "                text += p.extract_text() or ''\n",
    "    except Exception:\n",
    "        try:\n",
    "            uploaded.seek(0)\n",
    "        except Exception:\n",
    "            pass\n",
    "    finally:\n",
    "        try:\n",
    "            uploaded.seek(0)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return text\n",
    "\n",
    "def read_docx_file(uploaded):\n",
    "    text = ''\n",
    "    try:\n",
    "        import docx\n",
    "        doc = docx.Document(io.BytesIO(uploaded.read()))\n",
    "        text = '\\n'.join([p.text for p in doc.paragraphs])\n",
    "    except Exception:\n",
    "        pass\n",
    "    finally:\n",
    "        try:\n",
    "            uploaded.seek(0)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return text\n",
    "\n",
    "def extract_skills_simple(text_lower: str):\n",
    "    if not skills_list:\n",
    "        return []\n",
    "    found = []\n",
    "    for s in skills_list:\n",
    "        if s and s in text_lower:\n",
    "            found.append(s)\n",
    "    # dedupe preserve order\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for s in found:\n",
    "        if s not in seen:\n",
    "            out.append(s)\n",
    "            seen.add(s)\n",
    "    return out\n",
    "\n",
    "# Inference helpers\n",
    "def get_score(job_text: str, resume_text: str) -> float:\n",
    "    if not job_text or not resume_text:\n",
    "        return 0.0\n",
    "    job_emb = model.encode([job_text], convert_to_numpy=True)\n",
    "    resume_emb = model.encode([resume_text], convert_to_numpy=True)\n",
    "    return float(cosine_similarity(job_emb, resume_emb).squeeze())\n",
    "\n",
    "def justify(job_text: str, resume_text: str, k: int = 3):\n",
    "    job_sents = [s.strip() for s in re.split(r'[\\n\\.!?]+', job_text) if s.strip()][:200]\n",
    "    res_sents = [s.strip() for s in re.split(r'[\\n\\.!?]+', resume_text) if s.strip()][:200]\n",
    "    if not job_sents or not res_sents:\n",
    "        return []\n",
    "    ej = model.encode(job_sents, convert_to_numpy=True)\n",
    "    er = model.encode(res_sents, convert_to_numpy=True)\n",
    "    sim = cosine_similarity(er, ej).max(axis=1)\n",
    "    idx = np.argsort(sim)[-k:][::-1]\n",
    "    return [res_sents[i] for i in idx]\n",
    "\n",
    "uploaded_files = st.file_uploader('Upload resumes (.pdf, .docx, .txt) — multiple allowed', type=['pdf','docx','txt'], accept_multiple_files=True)\n",
    "\n",
    "if not job_text:\n",
    "    st.warning('Provide or select a job description first.')\n",
    "else:\n",
    "    if uploaded_files:\n",
    "        rows = []\n",
    "        for uf in uploaded_files:\n",
    "            name = uf.name\n",
    "            ext = name.split('.')[-1].lower()\n",
    "            if ext == 'txt':\n",
    "                resume_text = read_txt_file(uf)\n",
    "            elif ext == 'pdf':\n",
    "                resume_text = read_pdf_file(uf)\n",
    "            elif ext == 'docx':\n",
    "                resume_text = read_docx_file(uf)\n",
    "            else:\n",
    "                resume_text = ''\n",
    "            text_clean = normalize_text(resume_text)\n",
    "            score = get_score(job_text, text_clean)\n",
    "            matched = extract_skills_simple(text_clean.lower()) if text_clean else []\n",
    "            rows.append({\n",
    "                'file': name,\n",
    "                'chars': len(text_clean),\n",
    "                'matching score': round(score*100, 2),\n",
    "                'text': text_clean,\n",
    "            })\n",
    "        res = pd.DataFrame(rows).sort_values('matching score', ascending=False).reset_index(drop=True)\n",
    "        st.subheader('Top matches')\n",
    "        st.dataframe(res[['file','chars','matching score']].head(top_k), use_container_width=True)\n",
    "        st.download_button('Download results CSV', res.to_csv(index=False).encode('utf-8'), file_name='resume_screening_results.csv', mime='text/csv')\n",
    "\n",
    "        if show_just:\n",
    "            st.markdown('---')\n",
    "            st.subheader('Justifications')\n",
    "            top_subset = res.head(top_k)\n",
    "            for i, row in top_subset.iterrows():\n",
    "                with st.expander(f\"{row['file']} — Matching Score: {row['matching score']}%%\"):\n",
    "                    sents = justify(job_text, row['text'], k=just_n)\n",
    "                    if sents:\n",
    "                        for s in sents:\n",
    "                            st.write('- ', s)\n",
    "                    else:\n",
    "                        st.write('No sentences found.')\n",
    "    else:\n",
    "        st.info('Upload one or more resumes to score against the selected job.')\n",
    "\n",
    "\"\"\" % EMB_MODEL_NAME\n",
    "\n",
    "with open(\"resume_screening.py\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(streamlit_app)\n",
    "\n",
    "print('Wrote Streamlit app to resume_screening.py. Run it with: streamlit run resume_screening.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d16ed7",
   "metadata": {},
   "source": [
    "## 9) Summary\n",
    "\n",
    "This notebook built an end-to-end resume screening workflow:\n",
    "\n",
    "- Parsed resumes from PDF/DOCX/TXT/CSV.\n",
    "- Normalized and cleaned text, extracted basic entities (emails, phones, years), and matched a skills list via spaCy PhraseMatcher.\n",
    "- Loaded jobs and normalized title/description fields.\n",
    "- Computed sentence embeddings with an embeddings model, then ranked resumes by cosine similarity to job descriptions.\n",
    "- Displayed top matches, with matched skills and example sentences as lightweight justification.\n",
    "- Optional: trained a simple classifier if labeled pairs exist.\n",
    "- Generated a Streamlit for interactive screening.\n",
    "\n",
    "### Suggested next steps\n",
    "\n",
    "- Add OCR for scanned PDFs (e.g., pytesseract) and handle images inside PDFs.\n",
    "- Weight skills by recency or section (e.g., experience > summary > hobbies).\n",
    "- Multi-signal scoring: combine semantic similarity, skill overlap, years, and keyword boosts.\n",
    "- Cache embeddings to disk; store in a vector index (FAISS/Annoy) for fast retrieval.\n",
    "- Package with Docker; externalize models and data paths via env vars.\n",
    "  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
