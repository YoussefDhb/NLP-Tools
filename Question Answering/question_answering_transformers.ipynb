{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35457567",
   "metadata": {},
   "source": [
    "\n",
    "# Question Answering with Transformers\n",
    "**Goal:** Build and evaluate a Question Answering (QA) system using pretrained transformer models (like BERT, DistilBERT, RoBERTa, ALBERT).  \n",
    "This notebook supports:  \n",
    "- Loading SQuAD v1.1-style data or a CSV with `context`, `question`, `answers` columns.  \n",
    "- Running inference with pretrained QA models to extract answer spans.  \n",
    "- Evaluating using Exact Match (EM) and F1 (token-level) metrics.  \n",
    "- Comparing two models side-by-side.  \n",
    "- Fine-tuning a model on your dataset using the Hugging Face `Trainer`.  \n",
    "- Simple interactive interfaces: CLI and Streamlit examples.\n",
    "\n",
    "**Notes:** To run full experimentation and fine-tuning you will need a GPU and the `transformers`, `datasets`, and `evaluate` libraries installed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417d41e",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd41c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Uncomment if you need to install packages in your environment\n",
    "# %pip install transformers datasets evaluate pandas torch sentencepiece streamlit\n",
    "# %pip install git+https://github.com/huggingface/transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2828d4c1",
   "metadata": {},
   "source": [
    "## 2) Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83288b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\elevvo-mine\\myenv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, textwrap, warnings, math\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, pipeline, TrainingArguments, Trainer, default_data_collator\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "# Note: `load_metric` was moved to the `evaluate` package. Use `import evaluate; evaluate.load(\"squad\")` if needed.\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471b2b0f",
   "metadata": {},
   "source": [
    "## 3) Load SQuAD or custom CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db67ed2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No local SQuAD/CSV found. Attempting to load 'squad' via the datasets library (internet required).\n",
      "Loaded SQuAD subset via datasets (1000 samples).\n",
      "Samples loaded: 10000\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>To whom did the Virgin Mary allegedly appear i...</td>\n",
       "      <td>{'text': ['Saint Bernadette Soubirous'], 'answ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is in front of the Notre Dame Main Building?</td>\n",
       "      <td>{'text': ['a copper statue of Christ'], 'answe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>The Basilica of the Sacred heart at Notre Dame...</td>\n",
       "      <td>{'text': ['the Main Building'], 'answer_start'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What is the Grotto at Notre Dame?</td>\n",
       "      <td>{'text': ['a Marian place of prayer and reflec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Architecturally, the school has a Catholic cha...</td>\n",
       "      <td>What sits on top of the Main Building at Notre...</td>\n",
       "      <td>{'text': ['a golden statue of the Virgin Mary'...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Architecturally, the school has a Catholic cha...   \n",
       "1  Architecturally, the school has a Catholic cha...   \n",
       "2  Architecturally, the school has a Catholic cha...   \n",
       "3  Architecturally, the school has a Catholic cha...   \n",
       "4  Architecturally, the school has a Catholic cha...   \n",
       "\n",
       "                                            question  \\\n",
       "0  To whom did the Virgin Mary allegedly appear i...   \n",
       "1  What is in front of the Notre Dame Main Building?   \n",
       "2  The Basilica of the Sacred heart at Notre Dame...   \n",
       "3                  What is the Grotto at Notre Dame?   \n",
       "4  What sits on top of the Main Building at Notre...   \n",
       "\n",
       "                                             answers  \n",
       "0  {'text': ['Saint Bernadette Soubirous'], 'answ...  \n",
       "1  {'text': ['a copper statue of Christ'], 'answe...  \n",
       "2  {'text': ['the Main Building'], 'answer_start'...  \n",
       "3  {'text': ['a Marian place of prayer and reflec...  \n",
       "4  {'text': ['a golden statue of the Virgin Mary'...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# This cell tries to load local SQuAD-format JSON or a CSV with columns: context, question, answers\n",
    "def load_local_squad(json_path=\"train-v1.1.json\"):\n",
    "    if not os.path.exists(json_path):\n",
    "        return None\n",
    "    # Minimal parser for SQuAD v1.1 structure\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    rows = []\n",
    "    for article in data.get('data', []):\n",
    "        for para in article.get('paragraphs', []):\n",
    "            context = para.get('context', '')\n",
    "            for qa in para.get('qas', []):\n",
    "                q = qa.get('question', '')\n",
    "                answers = [a['text'] for a in qa.get('answers', [])] if 'answers' in qa else []\n",
    "                rows.append({'context': context, 'question': q, 'answers': answers})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "def load_dataset_csv(csv_path=\"squad_style.csv\"):\n",
    "    if not os.path.exists(csv_path):\n",
    "        return None\n",
    "    df = pd.read_csv(csv_path)\n",
    "    # Expect columns: context, question, answers (answers may be JSON list or string)\n",
    "    if 'answers' in df.columns:\n",
    "        # ensure list type\n",
    "        def parse_answers(x):\n",
    "            if isinstance(x, str):\n",
    "                try:\n",
    "                    v = json.loads(x)\n",
    "                    if isinstance(v, list): return v\n",
    "                except Exception:\n",
    "                    return [x]\n",
    "            if isinstance(x, list):\n",
    "                return x\n",
    "            return [str(x)]\n",
    "        df['answers'] = df['answers'].apply(parse_answers)\n",
    "    else:\n",
    "        df['answers'] = [[]]*len(df)\n",
    "    return df[['context','question','answers']]\n",
    "\n",
    "# Try local SQuAD, else CSV, else attempt to download small SQuAD via datasets\n",
    "df = load_local_squad(\"./QA/train-v1.1.json\") or load_dataset_csv(\"squad_style.csv\")\n",
    "if df is None:\n",
    "    print(\"No local SQuAD/CSV found. Attempting to load 'squad' via the datasets library (internet required).\")\n",
    "    try:\n",
    "        ds = load_dataset(\"squad\")\n",
    "        # convert to pandas for small subset; datasets structure different\n",
    "        train = ds['train'].select(range(10000))  # small subset for quick runs\n",
    "        df = pd.DataFrame({'context': train['context'], 'question': train['question'], 'answers': train['answers']})\n",
    "        print(\"Loaded SQuAD subset via datasets (10000 samples).\")\n",
    "    except Exception as e:\n",
    "        raise FileNotFoundError(\"No dataset found locally and failed to download SQuAD. Please provide 'train-v1.1.json' or 'squad_style.csv'.\\nError: \" + str(e))\n",
    "\n",
    "print(\"Samples loaded:\", len(df))\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18dde35",
   "metadata": {},
   "source": [
    "## 4) Helper: Exact Match & F1 scoring (SQuAD-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c61ec09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# SQuAD evaluation helpers (normalize answer & compute EM/F1)\n",
    "import string, re\n",
    "\n",
    "def normalize_answer(s):\n",
    "    def remove_articles(text):\n",
    "        return re.sub(r'\\b(a|an|the)\\b', ' ', text)\n",
    "    def white_space_fix(text):\n",
    "        return ' '.join(text.split())\n",
    "    def remove_punc(text):\n",
    "        return ''.join(ch for ch in text if ch not in set(string.punctuation))\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def f1_score(pred, truth):\n",
    "    pred_tokens = normalize_answer(pred).split()\n",
    "    truth_tokens = normalize_answer(truth).split()\n",
    "    common = Counter(pred_tokens) & Counter(truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = num_same / len(pred_tokens)\n",
    "    recall = num_same / len(truth_tokens)\n",
    "    return 2 * precision * recall / (precision + recall)\n",
    "\n",
    "def exact_match_score(pred, truth):\n",
    "    return int(normalize_answer(pred) == normalize_answer(truth))\n",
    "\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b968804b",
   "metadata": {},
   "source": [
    "## 5) Load two QA models for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33bbf6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: distilbert-base-uncased-distilled-squad\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ktrapeznikov/albert-xlarge-v2-squad-v2 were not used when initializing AlbertForQuestionAnswering: ['albert.pooler.bias', 'albert.pooler.weight']\n",
      "- This IS expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: ktrapeznikov/albert-xlarge-v2-squad-v2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['distilbert-base-uncased-distilled-squad',\n",
       " 'ktrapeznikov/albert-xlarge-v2-squad-v2']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Try to load two QA models (priority list). The notebook will use the first two available.\n",
    "CANDIDATE_QA_MODELS = [\n",
    "    \"distilbert-base-uncased-distilled-squad\",\n",
    "    \"ktrapeznikov/albert-xlarge-v2-squad-v2\",\n",
    "    \"bert-large-uncased-whole-word-masking-finetuned-squad\",\n",
    "    \"deepset/roberta-base-squad2\"\n",
    "]\n",
    "\n",
    "loaded_models = []\n",
    "for name in CANDIDATE_QA_MODELS:\n",
    "    try:\n",
    "        tok = AutoTokenizer.from_pretrained(name)\n",
    "        model = AutoModelForQuestionAnswering.from_pretrained(name).to(DEVICE)\n",
    "        loaded_models.append((name, tok, model))\n",
    "        print(\"Loaded:\", name)\n",
    "    except Exception as e:\n",
    "        print(\"Could not load\", name, \":\", e)\n",
    "    if len(loaded_models) >= 2:\n",
    "        break\n",
    "\n",
    "if not loaded_models:\n",
    "    raise RuntimeError(\"No QA models could be loaded. Install at least one of the candidate models.\")\n",
    "loaded_models_names = [m[0] for m in loaded_models]\n",
    "loaded_models_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2225582e",
   "metadata": {},
   "source": [
    "## 6) Inference: extract answer span for (context, question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc0dc55e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context (first 5000 chars):\n",
      " Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Model: distilbert-base-uncased-distilled-squad | Answer: Saint Bernadette Soubirous\n",
      "Model: ktrapeznikov/albert-xlarge-v2-squad-v2 | Answer:  Saint Bernadette Soubirous\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# For convenience, build HF pipelines (handles tokenization & span extraction)\n",
    "qa_pipelines = {}\n",
    "for name, tok, mdl in loaded_models:\n",
    "    try:\n",
    "        qa_pipelines[name] = pipeline(\"question-answering\", model=mdl, tokenizer=tok, device=0 if DEVICE=='cuda' else -1)\n",
    "    except Exception as e:\n",
    "        # fallback to manual inference\n",
    "        print(\"Pipeline creation failed for\", name, \":\", e)\n",
    "        qa_pipelines[name] = None\n",
    "\n",
    "def answer_with_pipeline(pipeline_obj, context, question):\n",
    "    if pipeline_obj is None:\n",
    "        return \"\"\n",
    "    out = pipeline_obj(question=question, context=context, top_k=1)\n",
    "    # pipeline may return dict or list\n",
    "    if isinstance(out, list):\n",
    "        out = out[0] if out else {}\n",
    "    return out.get('answer', ''), out\n",
    "\n",
    "# Quick inference on a small subset\n",
    "sample = df.iloc[0]\n",
    "print(\"Context (first 5000 chars):\\n\", sample['context'][:5000])\n",
    "print(\"\\nQuestion:\", sample['question'])\n",
    "for name, pipe in qa_pipelines.items():\n",
    "    ans, raw = answer_with_pipeline(pipe, sample['context'], sample['question'])\n",
    "    print(f\"Model: {name} | Answer: {ans}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d041cb",
   "metadata": {},
   "source": [
    "## 7) Batch inference & evaluation (EM / F1) on dataset subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d04c735d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run on a subset for speed; increase for full evaluation\n",
    "N = min(500, len(df))  # adjust for compute/time\n",
    "subset = df.iloc[:N].reset_index(drop=True)\n",
    "\n",
    "results = []\n",
    "for name, pipe in qa_pipelines.items():\n",
    "    print(\"Evaluating model:\", name)\n",
    "    em_total = 0.0\n",
    "    f1_total = 0.0\n",
    "    count = 0\n",
    "    preds = []\n",
    "    for i, row in subset.iterrows():\n",
    "        context = row['context']\n",
    "        question = row['question']\n",
    "        ans, raw = answer_with_pipeline(pipe, context, question)\n",
    "        # determine gold answer(s)\n",
    "        golds = row['answers'] if isinstance(row['answers'], list) else (row['answers'] or [])\n",
    "        # If no golds (e.g., CSV missing), skip metric computation\n",
    "        if not golds:\n",
    "            continue\n",
    "        # Compare to all golds, take max\n",
    "        em = max(exact_match_score(ans, g['text'] if isinstance(g, dict) else g) for g in golds)\n",
    "        f1 = max(f1_score(ans, g['text'] if isinstance(g, dict) else g) for g in golds)\n",
    "        em_total += em\n",
    "        f1_total += f1\n",
    "        count += 1\n",
    "        preds.append({'idx': i, 'pred': ans, 'gold': golds})\n",
    "    avg_em = em_total / max(1, count)\n",
    "    avg_f1 = f1_total / max(1, count)\n",
    "    results.append({'model': name, 'EM': avg_em, 'F1': avg_f1, 'count': count})\n",
    "    print(f\"Done {name} -> EM: {avg_em:.4f} | F1: {avg_f1:.4f} on {count} samples\")\n",
    "\n",
    "pd.DataFrame(results).sort_values('F1', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d228ef",
   "metadata": {},
   "source": [
    "## 8) Inspect predictions and failure cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d604989b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== Example 0 ====\n",
      "Question: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
      "Context (snippet): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "distilbert-base-uncased-distilled-squad -> Saint Bernadette Soubirous\n",
      "ktrapeznikov/albert-xlarge-v2-squad-v2 ->  Saint Bernadette Soubirous\n",
      "Gold: {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}\n",
      "\n",
      "==== Example 1 ====\n",
      "Question: What is in front of the Notre Dame Main Building?\n",
      "Context (snippet): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "distilbert-base-uncased-distilled-squad -> a copper statue of Christ\n",
      "ktrapeznikov/albert-xlarge-v2-squad-v2 ->  a copper statue of Christ\n",
      "Gold: {'text': ['a copper statue of Christ'], 'answer_start': [188]}\n",
      "\n",
      "==== Example 2 ====\n",
      "Question: The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
      "Context (snippet): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "distilbert-base-uncased-distilled-squad -> the Main Building\n",
      "ktrapeznikov/albert-xlarge-v2-squad-v2 ->  the Main Building\n",
      "Gold: {'text': ['the Main Building'], 'answer_start': [279]}\n",
      "\n",
      "==== Example 3 ====\n",
      "Question: What is the Grotto at Notre Dame?\n",
      "Context (snippet): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "distilbert-base-uncased-distilled-squad -> a Marian place of prayer and reflection\n",
      "ktrapeznikov/albert-xlarge-v2-squad-v2 ->  a Marian place of prayer and reflection\n",
      "Gold: {'text': ['a Marian place of prayer and reflection'], 'answer_start': [381]}\n",
      "\n",
      "==== Example 4 ====\n",
      "Question: What sits on top of the Main Building at Notre Dame?\n",
      "Context (snippet): Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "distilbert-base-uncased-distilled-squad -> a golden statue of the Virgin Mary\n",
      "ktrapeznikov/albert-xlarge-v2-squad-v2 ->  a golden statue of the Virgin Mary\n",
      "Gold: {'text': ['a golden statue of the Virgin Mary'], 'answer_start': [92]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show some examples where models disagree or predict poorly\n",
    "def show_examples(subset, qa_pipelines, n=5):\n",
    "    for i in range(min(n, len(subset))):\n",
    "        row = subset.iloc[i]\n",
    "        print(\"==== Example\", i, \"====\")\n",
    "        print(\"Question:\", row['question'])\n",
    "        print(\"Context (snippet):\", row['context'][:1000])\n",
    "        for name, pipe in qa_pipelines.items():\n",
    "            ans, raw = answer_with_pipeline(pipe, row['context'], row['question'])\n",
    "            print(f\"{name} -> {ans}\")\n",
    "        print(\"Gold:\", row['answers'])\n",
    "        print()\n",
    "\n",
    "show_examples(subset, qa_pipelines, n=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671a42c3",
   "metadata": {},
   "source": [
    "## 9) Fine-tuning template using Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c741eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:05<00:00, 1683.04 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared tokenized features for training.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# We'll use the Hugging Face datasets/dataloader conventions to prepare features for QA\n",
    "\n",
    "from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n",
    "from datasets import Dataset\n",
    "\n",
    "def _extract_first_answer(ans_obj):\n",
    "    \"\"\"Return (answer_text, answer_start) from various answer formats.\n",
    "    Supports:\n",
    "    - dict with 'text' (str or list) and optional 'answer_start' (int or list)\n",
    "    - list of dicts or strings\n",
    "    - plain string\n",
    "    Fallbacks to (-1) if no start provided; caller may compute via .find().\n",
    "    \"\"\"\n",
    "    if ans_obj is None:\n",
    "        return \"\", -1\n",
    "    # SQuAD-style dict\n",
    "    if isinstance(ans_obj, dict):\n",
    "        texts = ans_obj.get(\"text\", [])\n",
    "        starts = ans_obj.get(\"answer_start\", [])\n",
    "        # Normalize to first element\n",
    "        if isinstance(texts, list):\n",
    "            text0 = texts[0] if texts else \"\"\n",
    "        else:\n",
    "            text0 = str(texts)\n",
    "        if isinstance(starts, list):\n",
    "            start0 = starts[0] if starts else -1\n",
    "        else:\n",
    "            start0 = int(starts) if isinstance(starts, int) else -1\n",
    "        return text0, start0\n",
    "    # List of answers (strings or dicts)\n",
    "    if isinstance(ans_obj, list):\n",
    "        if not ans_obj:\n",
    "            return \"\", -1\n",
    "        first = ans_obj[0]\n",
    "        if isinstance(first, dict):\n",
    "            text0 = first.get(\"text\", \"\")\n",
    "            start0 = first.get(\"answer_start\", -1)\n",
    "            if isinstance(text0, list):\n",
    "                text0 = text0[0] if text0 else \"\"\n",
    "            if isinstance(start0, list):\n",
    "                start0 = start0[0] if start0 else -1\n",
    "            return str(text0), int(start0) if isinstance(start0, int) else -1\n",
    "        return str(first), -1\n",
    "    # Plain string\n",
    "    if isinstance(ans_obj, str):\n",
    "        return ans_obj, -1\n",
    "    return str(ans_obj), -1\n",
    "\n",
    "def prepare_train_features(examples, tokenizer, max_length=384, doc_stride=128):\n",
    "    # Tokenize our examples with truncation and maybe sliding window\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\"],\n",
    "        examples[\"context\"],\n",
    "        truncation=\"only_second\",\n",
    "        max_length=max_length,\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    # Map features to examples\n",
    "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
    "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        sample_index = sample_mapping[i]\n",
    "        context = examples[\"context\"][sample_index]\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "\n",
    "        # Extract first answer text/start (robust to formats)\n",
    "        answer_text, answer_start = _extract_first_answer(answers)\n",
    "        if not isinstance(context, str):\n",
    "            context = str(context)\n",
    "        if not isinstance(answer_text, str):\n",
    "            answer_text = str(answer_text)\n",
    "\n",
    "        if not answer_text:\n",
    "            # No answer provided\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "\n",
    "        # If start not provided, locate via substring search\n",
    "        if not isinstance(answer_start, int) or answer_start < 0:\n",
    "            answer_start = context.find(answer_text)\n",
    "        if answer_start < 0:\n",
    "            # Could not find answer in context\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "\n",
    "        answer_end = answer_start + len(answer_text)\n",
    "\n",
    "        # Identify the tokens that make up the context\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        # Find start and end of the context in tokenized input\n",
    "        token_start_index = 0\n",
    "        while token_start_index < len(sequence_ids) and sequence_ids[token_start_index] != 1:\n",
    "            token_start_index += 1\n",
    "        token_end_index = len(sequence_ids) - 1\n",
    "        while token_end_index >= 0 and sequence_ids[token_end_index] != 1:\n",
    "            token_end_index -= 1\n",
    "\n",
    "        # If answer not inside the context span, label as (0, 0)\n",
    "        if token_start_index >= len(offsets) or token_end_index < 0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "            continue\n",
    "\n",
    "        # Move token_start_index to the right to the first token that starts after answer_start\n",
    "        while token_start_index < len(offsets) and offsets[token_start_index] and offsets[token_start_index][0] <= answer_start:\n",
    "            token_start_index += 1\n",
    "        token_start_index -= 1\n",
    "\n",
    "        # Move token_end_index to the left to the last token that ends before answer_end\n",
    "        while token_end_index >= 0 and offsets[token_end_index] and offsets[token_end_index][1] >= answer_end:\n",
    "            token_end_index -= 1\n",
    "        token_end_index += 1\n",
    "\n",
    "        # Guard against invalid indices\n",
    "        if token_start_index < 0 or token_end_index < 0 or token_start_index >= len(offsets) or token_end_index >= len(offsets) or token_start_index > token_end_index:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            start_positions.append(token_start_index)\n",
    "            end_positions.append(token_end_index)\n",
    "\n",
    "    tokenized_examples[\"start_positions\"] = start_positions\n",
    "    tokenized_examples[\"end_positions\"] = end_positions\n",
    "    return tokenized_examples\n",
    "\n",
    "# Example usage: convert pandas df subset to datasets.Dataset\n",
    "if 'df' in globals():\n",
    "    # convert subset small for demo\n",
    "    demo = df.iloc[:10000].reset_index(drop=True).copy()\n",
    "    # Keep 'answers' as-is; the extractor handles dict/list formats\n",
    "    ds = Dataset.from_pandas(demo)\n",
    "    # select tokenizer and model for fine-tuning\n",
    "    model_name = loaded_models[0][0]\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForQuestionAnswering.from_pretrained(model_name).to(DEVICE)\n",
    "    tokenized = ds.map(lambda ex: prepare_train_features(ex, tokenizer), batched=True, remove_columns=ds.column_names)\n",
    "    tokenized.set_format(type=\"torch\")\n",
    "    print(\"Prepared tokenized features for training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25dc370",
   "metadata": {},
   "source": [
    "## 10) Interactive interfaces: CLI & Streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43c13cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote Streamlit example to QA_App.py\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CLI example: ask a question given a context in Python\n",
    "def answer_cli(context: str, question: str, model_name=None):\n",
    "    if model_name is None:\n",
    "        model_name = list(qa_pipelines.keys())[0]\n",
    "    pipe = qa_pipelines[model_name]\n",
    "    ans, raw = answer_with_pipeline(pipe, context, question)\n",
    "    print(\"Answer:\", ans)\n",
    "    return ans\n",
    "\n",
    "# Streamlit app (save as qa_app.py and run: streamlit run qa_app.py)\n",
    "streamlit_app = r\"\"\"\n",
    "import streamlit as st\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "st.set_page_config(page_title=\"QA App\")\n",
    "st.title('Question Answering')\n",
    "# Unique key to avoid duplicate element IDs\n",
    "model_name = st.sidebar.selectbox('Model', %s, key='model_select_sidebar')\n",
    "\n",
    "if st.button('Load model'):\n",
    "    # Second selectbox must have a different key\n",
    "    model_name_sel = st.sidebar.selectbox('Model', %s, key='model_select_loader')\n",
    "    st.write('Model Loaded')\n",
    "    device_index = 0 if torch.cuda.is_available() else -1\n",
    "    pipe = pipeline('question-answering', model=model_name_sel, tokenizer=model_name_sel, device=device_index)\n",
    "    st.session_state['pipe'] = pipe\n",
    "\n",
    "context = st.text_area('Context', height=300)\n",
    "question = st.text_input('Question')\n",
    "if st.button('Answer') and 'pipe' in st.session_state:\n",
    "    out = st.session_state['pipe'](question=question, context=context)\n",
    "    st.write('Answer:', out.get('answer'))\n",
    "\"\"\" % (loaded_models_names, loaded_models_names)\n",
    "\n",
    "with open(\"QA_App.py\",\"w\",encoding=\"utf-8\") as f:\n",
    "    f.write(streamlit_app)\n",
    "\n",
    "print(\"Wrote Streamlit example to QA_App.py\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7505d8b",
   "metadata": {},
   "source": [
    "## 11) Save predictions and results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "108114ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved qa_predictions_sample.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save sample predictions from first model to CSV\n",
    "if 'subset' in globals():\n",
    "    model0 = list(qa_pipelines.keys())[0]\n",
    "    preds = []\n",
    "    for i, row in subset.iterrows():\n",
    "        ans, raw = answer_with_pipeline(qa_pipelines[model0], row['context'], row['question'])\n",
    "        preds.append({'index': i, 'question': row['question'], 'prediction': ans, 'gold': row['answers']})\n",
    "    out_df = pd.DataFrame(preds)\n",
    "    out_df.to_csv('qa_predictions_sample.csv', index=False)\n",
    "    print('Saved qa_predictions_sample.csv')\n",
    "else:\n",
    "    print('No subset available to save predictions.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bac0f98",
   "metadata": {},
   "source": [
    "## 12) Summary & Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e0223b",
   "metadata": {},
   "source": [
    "\n",
    "### Summary\n",
    "- This notebook demonstrates loading SQuAD-style data, running pretrained QA models, and computing EM/F1 metrics.\n",
    "- It includes a minimal fine-tuning template and simple interactive CLI/Streamlit examples.\n",
    "- For production: consider model quantization, server deployment (FastAPI), batching, and caching tokenizers/models for low latency.\n",
    "\n",
    "### Next steps / improvements\n",
    "- Fine-tune on the full SQuAD dataset (requires GPU and time).\n",
    "- Add better handling of multiple gold answers and answer_start indices when preparing training data.\n",
    "- Use `evaluate` library's squad metric for robust evaluation and comparison.\n",
    "- Serve as a REST API with FastAPI + model pooling for scale.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
